{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:09.672625Z",
     "start_time": "2025-10-25T20:27:09.659231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import io\n",
    "from typing import Iterable, Callable\n",
    "import zipfile\n",
    "import traceback\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RawRepositoryFile:\n",
    "    filename: str\n",
    "    content: str\n",
    "\n",
    "\n",
    "class GithubRepositoryDataReader:\n",
    "    \"\"\"\n",
    "    Downloads and parses markdown and code files from a GitHub repository.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 repo_owner: str,\n",
    "                 repo_name: str,\n",
    "                 allowed_extensions: Iterable[str] | None = None,\n",
    "                 filename_filter: Callable[[str], bool] | None = None\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub repository data reader.\n",
    "\n",
    "        Args:\n",
    "            repo_owner: The owner/organization of the GitHub repository\n",
    "            repo_name: The name of the GitHub repository\n",
    "            allowed_extensions: Optional set of file extensions to include\n",
    "                    (e.g., {\"md\", \"py\"}). If not provided, all file types are included\n",
    "            filename_filter: Optional callable to filter files by their path\n",
    "        \"\"\"\n",
    "        prefix = \"https://codeload.github.com\"\n",
    "        self.url = (\n",
    "            f\"{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main\"\n",
    "        )\n",
    "\n",
    "        if allowed_extensions is not None:\n",
    "            self.allowed_extensions = {ext.lower() for ext in allowed_extensions}\n",
    "\n",
    "        if filename_filter is None:\n",
    "            self.filename_filter = lambda filepath: True\n",
    "        else:\n",
    "            self.filename_filter = filename_filter\n",
    "\n",
    "    def read(self) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Download and extract files from the GitHub repository.\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the repository download fails\n",
    "        \"\"\"\n",
    "        resp = requests.get(self.url)\n",
    "        if resp.status_code != 200:\n",
    "            raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "        zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "        repository_data = self._extract_files(zf)\n",
    "        zf.close()\n",
    "\n",
    "        return repository_data\n",
    "\n",
    "    def _extract_files(self, zf: zipfile.ZipFile) -> list[RawRepositoryFile]:\n",
    "        \"\"\"\n",
    "        Extract and process files from the zip archive.\n",
    "\n",
    "        Args:\n",
    "            zf: ZipFile object containing the repository data\n",
    "\n",
    "        Returns:\n",
    "            List of RawRepositoryFile objects for each processed file\n",
    "        \"\"\"\n",
    "        data = []\n",
    "\n",
    "        for file_info in zf.infolist():\n",
    "            filepath = self._normalize_filepath(file_info.filename)\n",
    "\n",
    "            if self._should_skip_file(filepath):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with zf.open(file_info) as f_in:\n",
    "                    content = f_in.read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    if content is not None:\n",
    "                        content = content.strip()\n",
    "\n",
    "                    file = RawRepositoryFile(\n",
    "                        filename=filepath,\n",
    "                        content=content\n",
    "                    )\n",
    "                    data.append(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_info.filename}: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _should_skip_file(self, filepath: str) -> bool:\n",
    "        \"\"\"\n",
    "        Determine whether a file should be skipped during processing.\n",
    "\n",
    "        Args:\n",
    "            filepath: The file path to check\n",
    "\n",
    "        Returns:\n",
    "            True if the file should be skipped, False otherwise\n",
    "        \"\"\"\n",
    "        filepath = filepath.lower()\n",
    "\n",
    "        # directory\n",
    "        if filepath.endswith(\"/\"):\n",
    "            return True\n",
    "\n",
    "        # hidden file\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\".\"):\n",
    "            return True\n",
    "\n",
    "        if self.allowed_extensions:\n",
    "            ext = self._get_extension(filepath)\n",
    "            if ext not in self.allowed_extensions:\n",
    "                return True\n",
    "\n",
    "        if not self.filename_filter(filepath):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _get_extension(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract the file extension from a filepath.\n",
    "\n",
    "        Args:\n",
    "            filepath: The file path to extract extension from\n",
    "\n",
    "        Returns:\n",
    "            The file extension (without dot) or empty string if no extension\n",
    "        \"\"\"\n",
    "        filename = filepath.lower().split(\"/\")[-1]\n",
    "        if \".\" in filename:\n",
    "            return filename.rsplit(\".\", maxsplit=1)[-1]\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def _normalize_filepath(self, filepath: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes the top-level directory from the file path inside the zip archive.\n",
    "        'repo-main/path/to/file.py' -> 'path/to/file.py'\n",
    "\n",
    "        Args:\n",
    "            filepath: The original filepath from the zip archive\n",
    "\n",
    "        Returns:\n",
    "            The normalized filepath with top-level directory removed\n",
    "        \"\"\"\n",
    "        parts = filepath.split(\"/\", maxsplit=1)\n",
    "        if len(parts) > 1:\n",
    "            return parts[1]\n",
    "        else:\n",
    "            return parts[0]\n"
   ],
   "id": "140bf99b717eb89a",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:09.681662Z",
     "start_time": "2025-10-25T20:27:09.679955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_github_data():\n",
    "    allowed_extensions = {\"md\", \"mdx\"}\n",
    "\n",
    "    repo_owner = 'evidentlyai'\n",
    "    repo_name = 'docs'\n",
    "\n",
    "    reader = GithubRepositoryDataReader(\n",
    "        repo_owner,\n",
    "        repo_name,\n",
    "        allowed_extensions=allowed_extensions\n",
    "    )\n",
    "\n",
    "    return reader.read()\n"
   ],
   "id": "9153b0b927bc3e80",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:10.885898Z",
     "start_time": "2025-10-25T20:27:09.686625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_raw = read_github_data()\n",
    "print(f\"Downloaded {len(data_raw)} files\")"
   ],
   "id": "9f43137f413ad4ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 95 files\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:10.907020Z",
     "start_time": "2025-10-25T20:27:10.902827Z"
    }
   },
   "cell_type": "code",
   "source": "data_raw[15]",
   "id": "a4799b140e187807",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RawRepositoryFile(filename='docs/library/tags_metadata.mdx', content='---\\ntitle: \\'Add tags and metadata\\'\\ndescription: \\'How to add metadata to evaluations.\\'\\n---\\n\\nThis is relevant when you logging Reports to the Platform. Tags help you associate each Report with a specific model / prompt version, time period, or other context.\\n\\n## Add timestamp\\n\\nEach Report run has a single timestamp. By default, Evidently assigns `datetime.now()` as the run time based on the user\\'s time zone.\\n\\nYou can also specify a custom timestamp by passing it to the `run()` method:\\n\\n```python\\nfrom datetime import datetime\\n\\nmy_eval_4 = report.run(eval_data_1,\\n                       eval_data_2,\\n                       timestamp=datetime(2024, 1, 29))\\n```\\n\\nBecause timestamps are fully customizable, you can log Reports asynchronously or with a delay. For example, make an evaluation after receiving ground truth and backdate Reports to the relevant time period.\\n\\n## Add tags and metadata\\n\\nYou can add `tags` and `metadata` to Reports to support search and ease of filtering. Tags also let you visualize data from specific subsets of Reports on monitoring Panels.\\n\\nUse tags in the following scenarios:\\n* Mark evaluation runs by model version, prompt version, or test scenario.\\n* Indicate status: production, shadow, champion/challenger, A/B versions.\\n* Identify Reports by geography, use case, user segment, or role.\\n* Tag based on reference dataset windows (for example, weekly vs. monthly drift comparisons)\\n* Highlight Reports with a specific role, such as datasheet or model card.\\n\\n**Custom tags**. You can add tags to the Report. Pass any custom Tags as a list:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\ntags=[\"classification\", \"production\"])\\n```\\n\\n**Custom metadata**. Pass metadata as a Python dictionary in key:value pairs:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\nmetadata = {\\n\\t\"deployment\": \"shadow\",\\n\\t\"status\": \"production\",\\n\\t})\\n```\\n\\n**Default metadata**. Use built-in metadata fields `model_id`, `reference_id`, `batch_size`, `dataset_id`:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n],\\n  model_id=\"model_id\",\\n\\treference_id=\"reference_id\",\\n\\tbatch_size=\"batch_size\",\\n\\tdataset_id=\"dataset_id\"\\n)\\n```\\n\\n**Add tags to run**: You can also tag individual Report runs. This is useful for experiments where you re-run the same Report with different prompts or hyperparameter settings.\\n\\n\\n```python\\nmy_eval = report.run(eval_data_1, eval_data_2, tags=[\"prompt_v1\", \"claude\"])\\n```')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:10.918182Z",
     "start_time": "2025-10-25T20:27:10.915188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Document chunking utilities for splitting large documents into smaller, overlapping pieces.\n",
    "\n",
    "This module provides functionality to break down documents into chunks using a sliding\n",
    "window approach, which is useful for processing large texts in smaller, manageable pieces\n",
    "while maintaining context through overlapping content.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any, Dict, Iterable, List\n",
    "\n",
    "\n",
    "def sliding_window(\n",
    "        seq: Iterable[Any],\n",
    "        size: int,\n",
    "        step: int\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Create overlapping chunks from a sequence using a sliding window approach.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence (string or list) to be chunked.\n",
    "        size (int): The size of each chunk/window.\n",
    "        step (int): The step size between consecutive windows.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing:\n",
    "            - 'start': The starting position of the chunk in the original sequence\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If size or step are not positive integers.\n",
    "\n",
    "    Example:\n",
    "        >>> sliding_window(\"hello world\", size=5, step=3)\n",
    "        [{'start': 0, 'content': 'hello'}, {'start': 3, 'content': 'lo wo'}]\n",
    "    \"\"\"\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        batch = seq[i:i+size]\n",
    "        result.append({'start': i, 'content': batch})\n",
    "        if i + size > n:\n",
    "            break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def chunk_documents(\n",
    "        documents: Iterable[Dict[str, str]],\n",
    "        size: int = 2000,\n",
    "        step: int = 1000,\n",
    "        content_field_name: str = 'content'\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Split a collection of documents into smaller chunks using sliding windows.\n",
    "\n",
    "    Takes documents and breaks their content into overlapping chunks while preserving\n",
    "    all other document metadata (filename, etc.) in each chunk.\n",
    "\n",
    "    Args:\n",
    "        documents: An iterable of document dictionaries. Each document must have a content field.\n",
    "        size (int, optional): The maximum size of each chunk. Defaults to 2000.\n",
    "        step (int, optional): The step size between chunks. Defaults to 1000.\n",
    "        content_field_name (str, optional): The name of the field containing document content.\n",
    "                                          Defaults to 'content'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunk dictionaries. Each chunk contains:\n",
    "            - All original document fields except the content field\n",
    "            - 'start': Starting position of the chunk in original content\n",
    "            - 'content': The chunk content\n",
    "\n",
    "    Example:\n",
    "        >>> documents = [{'content': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, size=100, step=50)\n",
    "        >>> # Or with custom content field:\n",
    "        >>> documents = [{'text': 'long text...', 'filename': 'doc.txt'}]\n",
    "        >>> chunks = chunk_documents(documents, content_field_name='text')\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_copy = doc.copy()\n",
    "        doc_content = doc_copy.pop(content_field_name)\n",
    "        chunks = sliding_window(doc_content, size=size, step=step)\n",
    "        for chunk in chunks:\n",
    "            chunk.update(doc_copy)\n",
    "        results.extend(chunks)\n",
    "\n",
    "    return results"
   ],
   "id": "24dd4ba95fc5b80c",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:10.926379Z",
     "start_time": "2025-10-25T20:27:10.924523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Document indexing utilities for creating searchable indexes from document collections.\n",
    "\n",
    "This module provides functionality to index documents using minsearch, with optional\n",
    "chunking support for handling large documents.\n",
    "\"\"\"\n",
    "\n",
    "from minsearch import Index\n",
    "\n",
    "def index_documents(documents, chunk: bool = False, chunking_params=None) -> Index:\n",
    "    \"\"\"\n",
    "    Create a searchable index from a collection of documents.\n",
    "\n",
    "    Args:\n",
    "        documents: A collection of document dictionaries, each containing at least\n",
    "                  'content' and 'filename' fields.\n",
    "        chunk (bool, optional): Whether to chunk documents before indexing.\n",
    "                               Defaults to False.\n",
    "        chunking_params (dict, optional): Parameters for document chunking.\n",
    "                                        Defaults to {'size': 2000, 'step': 1000}.\n",
    "                                        Only used when chunk=True.\n",
    "\n",
    "    Returns:\n",
    "        Index: A fitted minsearch Index object ready for searching.\n",
    "\n",
    "    Example:\n",
    "        >>> docs = [{'content': 'Hello world', 'filename': 'doc1.txt'}]\n",
    "        >>> index = index_documents(docs)\n",
    "        >>> results = index.search('hello')\n",
    "    \"\"\"\n",
    "    if chunk:\n",
    "        if chunking_params is None:\n",
    "            chunking_params = {'size': 2000, 'step': 1000}\n",
    "        documents = chunk_documents(documents, **chunking_params)\n",
    "\n",
    "    index = Index(\n",
    "        text_fields=[\"content\", \"filename\"],\n",
    "    )\n",
    "\n",
    "    index.fit(documents)\n",
    "    return index"
   ],
   "id": "eaf801419b3d98a2",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.279963Z",
     "start_time": "2025-10-25T20:27:10.929780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!uv add python-frontmatter\n",
    "!uv add rich"
   ],
   "id": "c95f0d1c0a252a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mResolved \u001B[1m154 packages\u001B[0m \u001B[2min 10ms\u001B[0m\u001B[0m\r\n",
      "\u001B[2mAudited \u001B[1m135 packages\u001B[0m \u001B[2min 0.07ms\u001B[0m\u001B[0m\r\n",
      "\u001B[2mResolved \u001B[1m154 packages\u001B[0m \u001B[2min 0.74ms\u001B[0m\u001B[0m\r\n",
      "\u001B[2mAudited \u001B[1m135 packages\u001B[0m \u001B[2min 0.03ms\u001B[0m\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.286255Z",
     "start_time": "2025-10-25T20:27:11.284142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import frontmatter\n",
    "from typing import List, Dict, Any\n",
    "from rich.progress import track\n",
    "\n",
    "def parse_data(data_raw: List[RawRepositoryFile]) -> List[Dict[str, Any]]:\n",
    "    print(\"ðŸ“„ [bold blue]Parsing documents...[/bold blue]\")\n",
    "\n",
    "    data_parsed = []\n",
    "    for f in track(data_raw, description=\"Processing files...\"):\n",
    "        post = frontmatter.loads(f.content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = f.filename\n",
    "        data_parsed.append(data)\n",
    "\n",
    "    return data_parsed"
   ],
   "id": "f42927b2aae12e6f",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.352281Z",
     "start_time": "2025-10-25T20:27:11.292186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = parse_data(data_raw)\n",
    "index = index_documents(\n",
    "    data,\n",
    "    chunk=True,\n",
    "    chunking_params={\"size\": 2000, \"step\": 1000},\n",
    ")"
   ],
   "id": "28d14278ec2b660c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cffeaca6a1b0457fabe17b12e8ebcc05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ [bold blue]Parsing documents...[/bold blue]\n"
     ]
    },
    {
     "data": {
      "text/plain": [],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.361625Z",
     "start_time": "2025-10-25T20:27:11.360422Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d0ad9006eb48aaf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.377017Z",
     "start_time": "2025-10-25T20:27:11.368083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index.search(\n",
    "    'How can I build an eval report with llm as a judge?',\n",
    "    num_results=15\n",
    ")"
   ],
   "id": "262174471cea531",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 1000,\n",
       "  'content': 'mply pass the selected Preset to the Report and run it over your data. If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. \\n\\n**Single dataset**. To generate the Data Summary Report for a single dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\nAfter you `run` the Report, the resulting `my_eval` will contains the computed values for each metric, along with associated metadata and visualizations. (We sometimes refer to this computation result as a `snapshot`).\\n\\n<Note>\\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n</Note>\\n\\n**Two datasets**. To generate reports like Data Drift that needs two datasets, pass the second one as a reference when you `run` it:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\nIn this case the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection. You can also pass it explicitly:\\n\\n```\\nmy_eval = report.run(current_data=eval_data_1, reference_data=eval_data_2)\\n```\\n\\n**Combine Presets**. You can also include multiple Presets in the same Report. List them one by one.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(), \\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n**Limit columns**. You can limit the columns to which the Preset is applied.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(column=[\"target\", \"prediction\"])\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n## Custom Report\\n\\n<Tip>\\n  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\n**Choose Metrics',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'start': 0,\n",
       "  'content': 'Reports perform evaluations on the Dataset level and/or summarize results of the row-level evaluations. For a general introduction, check [Core Concepts](/docs/library/overview).\\n\\n**Pre-requisites**:\\n\\n* You [installed Evidently](/docs/setup/installation).\\n\\n* You created a Dataset with the [Data Definition](/docs/library/data_definition).\\n\\n* (Optional) for text data, you added Descriptors.\\n\\n<Note>\\n  For a quick end-to-end example of generating Reports, check the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n</Note>\\n\\n## Imports\\n\\nImport the Metrics and Presets you plan to use.\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.presets import *\\n```\\n\\nYou can use Metric Presets, which are pre-built Reports that work out of the box, or create a custom Report selecting Metrics one by one.\\n\\n## Presets\\n\\n<Tip>\\n  **Available Presets**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\nTo generate a template Report, simply pass the selected Preset to the Report and run it over your data. If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. \\n\\n**Single dataset**. To generate the Data Summary Report for a single dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\nAfter you `run` the Report, the resulting `my_eval` will contains the computed values for each metric, along with associated metadata and visualizations. (We sometimes refer to this computation result as a `snapshot`).\\n\\n<Note>\\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n</Note>\\n\\n**Two datasets**. To generate reports like Data Drift that needs two datasets, pass the second one as a reference when you `run` it:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset()\\n])\\n\\nmy_eval = re',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'start': 2000,\n",
       "  'content': 'port.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\nIn this case the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection. You can also pass it explicitly:\\n\\n```\\nmy_eval = report.run(current_data=eval_data_1, reference_data=eval_data_2)\\n```\\n\\n**Combine Presets**. You can also include multiple Presets in the same Report. List them one by one.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(), \\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n**Limit columns**. You can limit the columns to which the Preset is applied.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(column=[\"target\", \"prediction\"])\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n## Custom Report\\n\\n<Tip>\\n  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\n**Choose Metrics**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Presets and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.\\n\\n```python\\nreport = Report([\\n    ColumnCount(), \\n    ValueStats(column=\"target\")\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\n<Note>\\n  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).\\n</Note>\\n\\n**Metric Parameters**. Metrics can have optional or required parameters.\\n\\nFor example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).\\n\\n```python\\nreport = Report([\\n   ValueDrift(column=\"target\", method=\"psi\")\\n])\\n```\\n\\nTo calculate the Precision at K for a ranking task, ',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'start': 3000,\n",
       "  'content': '**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Presets and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.\\n\\n```python\\nreport = Report([\\n    ColumnCount(), \\n    ValueStats(column=\"target\")\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\n<Note>\\n  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).\\n</Note>\\n\\n**Metric Parameters**. Metrics can have optional or required parameters.\\n\\nFor example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).\\n\\n```python\\nreport = Report([\\n   ValueDrift(column=\"target\", method=\"psi\")\\n])\\n```\\n\\nTo calculate the Precision at K for a ranking task, you must always pass the `k` parameter (Required).\\n\\n```python\\nreport = Report([\\n   PrecisionTopK(k=10)\\n])\\n```\\n\\n## Compare results\\n\\nIf you computed multiple snapshots, you can quickly compare the resulting metrics side-by-side in a dataframe:\\n\\n```python\\nfrom evidently import compare\\n\\ncompare_dataframe = compare(my_eval_1, my_eval_2, my_eval_3)\\n```\\n\\n## Group by\\n\\nYou can calculate metrics separately for different groups in your data, using a column with categories to split by. Use the `GroupyBy` metric as shown below.\\n\\n**Example**. This will compute the maximum value of salaries by each label in the \"Department\" column.\\n\\n```python\\nfrom evidently.metrics.group_by import GroupBy\\n\\nreport = Report([\\n    GroupBy(MaxValue(column=\"Salary\"), \"Department\"),\\n])\\nmy_eval = report.run(data, None)\\nmy_eval.dict()\\n```\\n\\nNote: you cannot use auto-generated Test conditions when you use GroupBy.\\n\\n## What\\'s next?\\n\\nYou can also add conditions to Metrics: check the [Tests guide](/docs/library/tests).',\n",
       "  'title': 'Report',\n",
       "  'description': 'How to generate Report.',\n",
       "  'filename': 'docs/library/report.mdx'},\n",
       " {'start': 20000,\n",
       "  'content': 'openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 0,\n",
       "  'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, weâ€™ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter ',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 0,\n",
       "  'content': \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\xa0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\",\n",
       "  'title': 'LLM regression testing',\n",
       "  'description': 'How to run regression testing for LLM outputs.',\n",
       "  'filename': 'examples/LLM_regression_testing.mdx'},\n",
       " {'start': 16000,\n",
       "  'content': 'view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFra',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 3000,\n",
       "  'content': ' Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe\\'ll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we\\'ll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n<Info>\\n  If you don\\'t have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        DeclineLLMEval(\"answer\", alias=\"Denials\")]) \\n\\n# Or IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\")\\n```\\n\\n**Congratulations\\\\!** You\\'ve just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in t',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'start': 1000,\n",
       "  'content': 'n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 8000,\n",
       "  'content': 'n LLM judge templates.\\n\\n<Accordion title=\"Custom LLM judge\" description=\"How to create a custom LLM evaluator\" icon=\"sparkles\">\\n  Let\\'s classify user questions as \"appropriate\" or \"inappropriate\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\"\"\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\"\"\",\\n      target_category=\"APPROPRIATE\",\\n      non_target_category=\"INAPPROPRIATE\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\"question\", template=appropriate_scope,\\n                  provider=\"openai\", model=\"gpt-4o-mini\",\\n                  alias=\"Question topic\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What\\'s next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).',\n",
       "  'title': 'LLM Evaluation',\n",
       "  'description': 'Evaluate text outputs in under 5 minutes',\n",
       "  'filename': 'quickstart_llm.mdx'},\n",
       " {'start': 5000,\n",
       "  'content': 'API. Here is how generating a Report with data summary preset for a single dataset works now:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\n```\\n\\nKey changes:\\n\\n- The Report object now defines the configuration (e.g., metrics to include).\\n- Running a Report returns a separate result object.\\n\\n<Card title=\"Reports\" href=\"/docs/library/report\">\\n  How to generate Reports.\\n</Card>\\n\\nAdditional improvement: you can also now use \"Group by\" to compute metrics for specific segments.\\n\\n### Test Suites joined with Reports\\n\\nMost importantly,  Reports and Tests are now unified. Previously, these were separate:\\n\\n- Reports provided an overview of metrics (e.g., distribution summaries, statistics).\\n- Tests verify pass/fail conditions (e.g., check for missing data or LLM quality thresholds).\\n\\nNow, the Test Suite mode is an optional extension of a Report. If you choose to enable Tests, their results appear as a separate tab in the same HTML file. This eliminated duplication and the need to switch between separate files or Reports.\\n\\nFor example, here is how you add a Test on max length that will appear in the same Report as all data / column statistics.\\n\\n```python\\nreport = Report([\\n     DataSummaryPreset(),\\n     MaxValue(column=\"Length\", tests=[lt(100)]),\\n])\\n```\\n\\nYou can still use auto-generated Test conditions based on your reference dataset or define your own expectations.\\n\\n<Card title=\"Tests\" href=\"/docs/library/tests\">\\n  How to add Tests with conditions.\\n</Card>\\n\\n### Metric redesign\\n\\nThe Metric object has been simplified:\\n\\n- Metrics now produce a single computation result with a fixed structure.\\n- Some visualization types can be specified directly as parameters to the Metric.\\n\\nThis redesign significantly improves JSON result parsing and UI integration, since each Metric has a single or two results only.\\n\\nYou can check the list of new Me',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'start': 17000,\n",
       "  'content': '\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, letâ€™s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you donâ€™t have a reference answer.\\n\\nHere\\'s ho',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'},\n",
       " {'start': 4000,\n",
       "  'content': 'e(df),\\n    data_definition=DataDefinition(\\n        text_columns=[\"question\", \"answer\"]),\\n    descriptors=[\\n        Sentiment(\"answer\", alias=\"Sentiment\"),\\n        TextLength(\"answer\", alias=\"Length\"),\\n        IncludesWords(\"answer\", words_list=[\\'sorry\\', \\'apologize\\'], alias=\"Denials\"),\\n    ]\\n)\\n```\\n\\n**2. Aggregate results or run conditional checks**. Use these descriptors like any other dataset column when creating a Report. For example, here is how you summarize all descriptors and check that the text length is under 100 symbols.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\"Length\", tests=[lt(100)]),\\n])\\n```\\n\\nThis decoupling means you can reuse descriptor outputs for multiple tests or aggregations without recomputation. Itâ€™s especially useful for LLM evaluations.\\n\\n<Card title=\"Descriptors\" href=\"/docs/library/descriptors\">\\n  Docs on adding descriptors.\\n</Card>\\n\\n### New Reports API\\n\\nAs you may have noticed in the example above, we made the changes to the core Report API. Here is how generating a Report with data summary preset for a single dataset works now:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\n```\\n\\nKey changes:\\n\\n- The Report object now defines the configuration (e.g., metrics to include).\\n- Running a Report returns a separate result object.\\n\\n<Card title=\"Reports\" href=\"/docs/library/report\">\\n  How to generate Reports.\\n</Card>\\n\\nAdditional improvement: you can also now use \"Group by\" to compute metrics for specific segments.\\n\\n### Test Suites joined with Reports\\n\\nMost importantly,  Reports and Tests are now unified. Previously, these were separate:\\n\\n- Reports provided an overview of metrics (e.g., distribution summaries, statistics).\\n- Tests verify pass/fail conditions (e.g., check for missing data or LLM quality thresholds).\\n\\nNow, the Test Suite mode is an optional extension of a Report. If ',\n",
       "  'title': 'Migration Guide',\n",
       "  'description': 'How to migrate to the new Evidently version?',\n",
       "  'filename': 'faq/migration.mdx'},\n",
       " {'start': 2000,\n",
       "  'content': 'notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  ',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx'}]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.412305Z",
     "start_time": "2025-10-25T20:27:11.410785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(query):\n",
    "    return index.search(\n",
    "        query=query,\n",
    "        num_results=15\n",
    "    )"
   ],
   "id": "6bdaf85a54a16446",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.421989Z",
     "start_time": "2025-10-25T20:27:11.420384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "instructions = \"\"\"\n",
    "You're an assistant that helps with the documentation.\n",
    "Answer the QUESTION based on the CONTEXT from the search engine of our documentation.\n",
    "\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "When answering the question, provide the reference to the file with the source.\n",
    "Use the filename field for that.\n",
    "The repo url is: https://github.com/evidentlyai/docs/\n",
    "\n",
    "Include code examples when relevant.\n",
    "If the question is discussed in multiple documents, cite all of them.\n",
    "\n",
    "Don't use markdown or any formatting in the output.\n",
    "\"\"\".strip()\n",
    "\n"
   ],
   "id": "7911b050a5f61753",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.427791Z",
     "start_time": "2025-10-25T20:27:11.426161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results)\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ).strip()\n",
    "\n",
    "    return prompt\n"
   ],
   "id": "1e29aef1dc374ce7",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.447591Z",
     "start_time": "2025-10-25T20:27:11.432879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Interact with LLM\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def interact_with_llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ],
   "id": "aa4b4ef283aeb6",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:11.453206Z",
     "start_time": "2025-10-25T20:27:11.451633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ask_evidently(query):\n",
    "    search_results = search(query)\n",
    "    user_prompt = build_prompt(query,search_results)\n",
    "    print(user_prompt)\n",
    "    response = interact_with_llm(user_prompt,instructions)\n",
    "    return response"
   ],
   "id": "83d530ec963f417c",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:22.586460Z",
     "start_time": "2025-10-25T20:27:11.457242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = ask_evidently('How can I build an eval report with llm as a judge?')\n",
    "print(result)"
   ],
   "id": "1d56a447bba2315b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<QUESTION>\n",
      "How can I build an eval report with llm as a judge?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "[{\"start\": 1000, \"content\": \"mply pass the selected Preset to the Report and run it over your data. If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. \\n\\n**Single dataset**. To generate the Data Summary Report for a single dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\nAfter you `run` the Report, the resulting `my_eval` will contains the computed values for each metric, along with associated metadata and visualizations. (We sometimes refer to this computation result as a `snapshot`).\\n\\n<Note>\\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n</Note>\\n\\n**Two datasets**. To generate reports like Data Drift that needs two datasets, pass the second one as a reference when you `run` it:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\nIn this case the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection. You can also pass it explicitly:\\n\\n```\\nmy_eval = report.run(current_data=eval_data_1, reference_data=eval_data_2)\\n```\\n\\n**Combine Presets**. You can also include multiple Presets in the same Report. List them one by one.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(), \\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n**Limit columns**. You can limit the columns to which the Preset is applied.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(column=[\\\"target\\\", \\\"prediction\\\"])\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n## Custom Report\\n\\n<Tip>\\n  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\n**Choose Metrics\", \"title\": \"Report\", \"description\": \"How to generate Report.\", \"filename\": \"docs/library/report.mdx\"}, {\"start\": 0, \"content\": \"Reports perform evaluations on the Dataset level and/or summarize results of the row-level evaluations. For a general introduction, check [Core Concepts](/docs/library/overview).\\n\\n**Pre-requisites**:\\n\\n* You [installed Evidently](/docs/setup/installation).\\n\\n* You created a Dataset with the [Data Definition](/docs/library/data_definition).\\n\\n* (Optional) for text data, you added Descriptors.\\n\\n<Note>\\n  For a quick end-to-end example of generating Reports, check the Quickstart [for ML](/quickstart_ml) or [LLM](/quickstart_llm).\\n</Note>\\n\\n## Imports\\n\\nImport the Metrics and Presets you plan to use.\\n\\n```python\\nfrom evidently import Report\\nfrom evidently.metrics import *\\nfrom evidently.presets import *\\n```\\n\\nYou can use Metric Presets, which are pre-built Reports that work out of the box, or create a custom Report selecting Metrics one by one.\\n\\n## Presets\\n\\n<Tip>\\n  **Available Presets**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\nTo generate a template Report, simply pass the selected Preset to the Report and run it over your data. If nothing else is specified, the Report will run with the default parameters for all columns in the dataset. \\n\\n**Single dataset**. To generate the Data Summary Report for a single dataset:\\n\\n```python\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\nAfter you `run` the Report, the resulting `my_eval` will contains the computed values for each metric, along with associated metadata and visualizations. (We sometimes refer to this computation result as a `snapshot`).\\n\\n<Note>\\nYou can render the results in Python, export as HTML, JSON or Python dictionary or upload to the Evidently platform. Check more in [output formats](/docs/library/output_formats).\\n</Note>\\n\\n**Two datasets**. To generate reports like Data Drift that needs two datasets, pass the second one as a reference when you `run` it:\\n\\n```python\\nreport = Report([\\n    DataDriftPreset()\\n])\\n\\nmy_eval = re\", \"title\": \"Report\", \"description\": \"How to generate Report.\", \"filename\": \"docs/library/report.mdx\"}, {\"start\": 2000, \"content\": \"port.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\nIn this case the first `eval_data_1` is the current data you evaluate, the second `eval_data_2` is the reference dataset you consider as a baseline for drift detection. You can also pass it explicitly:\\n\\n```\\nmy_eval = report.run(current_data=eval_data_1, reference_data=eval_data_2)\\n```\\n\\n**Combine Presets**. You can also include multiple Presets in the same Report. List them one by one.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(), \\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n**Limit columns**. You can limit the columns to which the Preset is applied.\\n\\n```python\\nreport = Report([\\n    DataDriftPreset(column=[\\\"target\\\", \\\"prediction\\\"])\\n])\\n\\nmy_eval = report.run(eval_data_1, eval_data_2)\\nmy_eval\\n#my_eval.json\\n```\\n\\n## Custom Report\\n\\n<Tip>\\n  **Available Metrics and parameters**. Check available evals in the [Reference table](/metrics/all_metrics).\\n</Tip>\\n\\n**Choose Metrics**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Presets and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.\\n\\n```python\\nreport = Report([\\n    ColumnCount(), \\n    ValueStats(column=\\\"target\\\")\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\n<Note>\\n  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).\\n</Note>\\n\\n**Metric Parameters**. Metrics can have optional or required parameters.\\n\\nFor example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).\\n\\n```python\\nreport = Report([\\n   ValueDrift(column=\\\"target\\\", method=\\\"psi\\\")\\n])\\n```\\n\\nTo calculate the Precision at K for a ranking task, \", \"title\": \"Report\", \"description\": \"How to generate Report.\", \"filename\": \"docs/library/report.mdx\"}, {\"start\": 3000, \"content\": \"**. To create a custom Report, simply list the Metics one by one. You can combine both dataset-level and column-level Metrics, and combine Presets and Metrics in one Report. When you use a column-level Metric, you must specify the column it refers to.\\n\\n```python\\nreport = Report([\\n    ColumnCount(), \\n    ValueStats(column=\\\"target\\\")\\n])\\n\\nmy_eval = report.run(eval_data_1, None)\\nmy_eval\\n#my_eval.json\\n```\\n\\n<Note>\\n  **Generating multiple column-level Metrics**: You can use a helper function to easily generate multiple column-level Metrics for a list of columns. See the page on [Metric Generator](/docs/library/metric_generator).\\n</Note>\\n\\n**Metric Parameters**. Metrics can have optional or required parameters.\\n\\nFor example, the data drift detection algorithm automatically selects a method, but you can override this by specifying your preferred method (Optional).\\n\\n```python\\nreport = Report([\\n   ValueDrift(column=\\\"target\\\", method=\\\"psi\\\")\\n])\\n```\\n\\nTo calculate the Precision at K for a ranking task, you must always pass the `k` parameter (Required).\\n\\n```python\\nreport = Report([\\n   PrecisionTopK(k=10)\\n])\\n```\\n\\n## Compare results\\n\\nIf you computed multiple snapshots, you can quickly compare the resulting metrics side-by-side in a dataframe:\\n\\n```python\\nfrom evidently import compare\\n\\ncompare_dataframe = compare(my_eval_1, my_eval_2, my_eval_3)\\n```\\n\\n## Group by\\n\\nYou can calculate metrics separately for different groups in your data, using a column with categories to split by. Use the `GroupyBy` metric as shown below.\\n\\n**Example**. This will compute the maximum value of salaries by each label in the \\\"Department\\\" column.\\n\\n```python\\nfrom evidently.metrics.group_by import GroupBy\\n\\nreport = Report([\\n    GroupBy(MaxValue(column=\\\"Salary\\\"), \\\"Department\\\"),\\n])\\nmy_eval = report.run(data, None)\\nmy_eval.dict()\\n```\\n\\nNote: you cannot use auto-generated Test conditions when you use GroupBy.\\n\\n## What's next?\\n\\nYou can also add conditions to Metrics: check the [Tests guide](/docs/library/tests).\", \"title\": \"Report\", \"description\": \"How to generate Report.\", \"filename\": \"docs/library/report.mdx\"}, {\"start\": 20000, \"content\": \"openai\\\",\\n            model = \\\"gpt-4o-mini\\\",\\n            alias=\\\"Verbosity\\\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\u00a0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don't fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you've got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What's next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 0, \"content\": \"import CloudSignup from '/snippets/cloud_signup.mdx';\\nimport CreateProject from '/snippets/create_project.mdx';\\n\\nIn this tutorial, we'll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we\\u2019ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe'll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \\\"ground truth\\\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there's no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere's what we'll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge's evaluations with manual labels.\\n\\nWe'll start with the reference-based evaluator that determines whether a new response is correct (it's more complex since it requires passing two columns to the prompt). Then, we'll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter \", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 0, \"content\": \"In this tutorial, you will learn how to perform regression testing for LLM outputs.\\n\\nYou can compare new and old responses after changing a prompt, model, or anything else in your system. By re-running the same inputs with new parameters, you can spot any significant changes. This helps you push updates with confidence or identify issues to fix.\\n\\n<Info>\\n  **This example uses Evidently Cloud.** You'll run evals in Python and upload them. You can also skip the upload and view Reports locally. For self-hosted, replace `CloudWorkspace` with `Workspace`.\\n</Info>\\n\\n# Tutorial scope\\n\\nHere's what we'll do:\\n\\n* **Create a toy dataset**. Build a small Q&A dataset with answers and reference responses.\\n\\n* **Get new answers**. Imitate generating new answers to the same question.\\n\\n* **Create and run a Report with Tests**. Compare the answers using LLM-as-a-judge to evaluate length, correctness and style consistency.\\n\\n* **Build a monitoring Dashboard**. Get plots to track the results of Tests over time.\\n\\n<Note>\\n  To simplify things, we won't create an actual LLM app, but will simulate generating new outputs.\\n</Note>\\n\\nTo complete the tutorial, you will need:\\n\\n* Basic Python knowledge.\\u00a0\\n\\n* An OpenAI API key to use for the LLM evaluator.\\n\\n* An Evidently Cloud account to track test results. If not yet, [sign up](https://www.evidentlyai.com/register) for a free account.\\n\\n<Info>\\n  You can see all the code in [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb) or click to [open in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/Regression_testing_with_debugging_updated.ipynb).\\n</Info>\\n\\n## 1. Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently[llm] \\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nfrom evidently.future.datasets import Dataset\\nfrom evidently.future.datasets import DataDefinition\\nfrom evidently.futur\", \"title\": \"LLM regression testing\", \"description\": \"How to run regression testing for LLM outputs.\", \"filename\": \"examples/LLM_regression_testing.mdx\"}, {\"start\": 16000, \"content\": \"view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\\\"label\\\", \\\"Correctness\\\"], alias=\\\"Judge_match\\\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \\\"incorrect\\\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we're going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \\\"target\\\", and the LLM-judge response is the \\\"prediction\\\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\\\"label\\\",\\n        prediction_labels=\\\"Correctness\\\",\\n        pos_label = \\\"incorrect\\\")],\\n    categorical_columns=[\\\"label\\\", \\\"Correctness\\\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFra\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 3000, \"content\": \" Inputs, context, and outputs (for RAG evaluation)\\n</Info>\\n\\n<Info>\\n  **Collecting live data**. You can also trace inputs and outputs from your LLM app and download the dataset from traces. See the [Tracing Quickstart](/quickstart_tracing)\\n</Info>\\n\\n## 3. Run evaluations\\n\\nWe'll evaluate the answers for:\\n\\n- **Sentiment:** from -1 (negative) to 1 (positive)\\n- **Text length:** character count\\n- **Denials:** refusals to answer. This uses an LLM-as-a-judge with built-in prompt.\\n\\nEach evaluation is a `descriptor`. It adds a new score or label to each row in your dataset.\\n\\nFor LLM-as-a-judge, we'll use OpenAI GPT-4o mini. Set OpenAI key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\\\"OPENAI_API_KEY\\\"] = \\\"YOUR KEY\\\"\\n```\\n\\n<Info>\\n  If you don't have an OpenAI key, you can use a keyword-based check `IncludesWords` instead.\\n</Info>\\n\\nTo run evals, pass the dataset and specify the list of descriptors to add:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        Sentiment(\\\"answer\\\", alias=\\\"Sentiment\\\"),\\n        TextLength(\\\"answer\\\", alias=\\\"Length\\\"),\\n        DeclineLLMEval(\\\"answer\\\", alias=\\\"Denials\\\")]) \\n\\n# Or IncludesWords(\\\"answer\\\", words_list=['sorry', 'apologize'], alias=\\\"Denials\\\")\\n```\\n\\n**Congratulations\\\\!** You've just run your first eval. Preview the results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_quickstart_preview.png)\\n\\n<Info>\\n  **What other evals are there?** Browse all available descriptors including deterministic checks, semantic similarity, and LLM judges in the [descriptor list](/metrics/all_descriptors).\\n</Info>\\n\\n## 4.  Create a Report\\n\\n**Create and run a Report**. It will summarize the evaluation results. \\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\n```\\n\\n**Local preview**. In a Python environment like Jupyter notebook or Colab, run:\\n\\n```python\\nmy_eval\\n```\\n\\nThis will render the Report directly in t\", \"title\": \"LLM Evaluation\", \"description\": \"Evaluate text outputs in under 5 minutes\", \"filename\": \"quickstart_llm.mdx\"}, {\"start\": 1000, \"content\": \"n.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere's what we'll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge's evaluations with manual labels.\\n\\nWe'll start with the reference-based evaluator that determines whether a new response is correct (it's more complex since it requires passing two columns to the prompt). Then, we'll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\\\"OPENAI_API_KEY\\\"] = \\\"YOUR_KEY\\\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 8000, \"content\": \"n LLM judge templates.\\n\\n<Accordion title=\\\"Custom LLM judge\\\" description=\\\"How to create a custom LLM evaluator\\\" icon=\\\"sparkles\\\">\\n  Let's classify user questions as \\\"appropriate\\\" or \\\"inappropriate\\\" for an educational tool.\\n\\n  ```python\\n  # Define the evaluation criteria\\n  appropriate_scope = BinaryClassificationPromptTemplate(\\n      criteria=\\\"\\\"\\\"An appropriate question is any educational query related to\\n      academic subjects, general school-level world knowledge, or skills.\\n      An inappropriate question is anything offensive, irrelevant, or out of\\n      scope.\\\"\\\"\\\",\\n      target_category=\\\"APPROPRIATE\\\",\\n      non_target_category=\\\"INAPPROPRIATE\\\",\\n      include_reasoning=True,\\n  )\\n  \\n  # Apply evaluation\\n  llm_evals = Dataset.from_pandas(\\n      eval_df,\\n      data_definition=DataDefinition(),\\n      descriptors=[\\n          LLMEval(\\\"question\\\", template=appropriate_scope,\\n                  provider=\\\"openai\\\", model=\\\"gpt-4o-mini\\\",\\n                  alias=\\\"Question topic\\\")\\n      ]\\n  )\\n  \\n  # Run and upload report\\n  report = Report([\\n      TextEvals()\\n  ])\\n  \\n  my_eval = report.run(llm_evals, None)\\n  ws.add_run(project.id, my_eval, include_data=True)\\n  \\n  # Uncomment to replace ws.add_run for a local preview \\n  # my_eval\\n  ```\\n\\n  You can implement any criteria this way, and plug in different LLM models.\\n</Accordion>\\n\\n![](/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png)\\n\\n## What's next?\\n\\nRead more on how you can configure [LLM judges for custom criteria or using other LLMs](/metrics/customize_llm_judge).\\n\\nWe also have lots of other examples\\\\! [Explore tutorials](/metrics/introduction).\", \"title\": \"LLM Evaluation\", \"description\": \"Evaluate text outputs in under 5 minutes\", \"filename\": \"quickstart_llm.mdx\"}, {\"start\": 5000, \"content\": \"API. Here is how generating a Report with data summary preset for a single dataset works now:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\n```\\n\\nKey changes:\\n\\n- The Report object now defines the configuration (e.g., metrics to include).\\n- Running a Report returns a separate result object.\\n\\n<Card title=\\\"Reports\\\" href=\\\"/docs/library/report\\\">\\n  How to generate Reports.\\n</Card>\\n\\nAdditional improvement: you can also now use \\\"Group by\\\" to compute metrics for specific segments.\\n\\n### Test Suites joined with Reports\\n\\nMost importantly,  Reports and Tests are now unified. Previously, these were separate:\\n\\n- Reports provided an overview of metrics (e.g., distribution summaries, statistics).\\n- Tests verify pass/fail conditions (e.g., check for missing data or LLM quality thresholds).\\n\\nNow, the Test Suite mode is an optional extension of a Report. If you choose to enable Tests, their results appear as a separate tab in the same HTML file. This eliminated duplication and the need to switch between separate files or Reports.\\n\\nFor example, here is how you add a Test on max length that will appear in the same Report as all data / column statistics.\\n\\n```python\\nreport = Report([\\n     DataSummaryPreset(),\\n     MaxValue(column=\\\"Length\\\", tests=[lt(100)]),\\n])\\n```\\n\\nYou can still use auto-generated Test conditions based on your reference dataset or define your own expectations.\\n\\n<Card title=\\\"Tests\\\" href=\\\"/docs/library/tests\\\">\\n  How to add Tests with conditions.\\n</Card>\\n\\n### Metric redesign\\n\\nThe Metric object has been simplified:\\n\\n- Metrics now produce a single computation result with a fixed structure.\\n- Some visualization types can be specified directly as parameters to the Metric.\\n\\nThis redesign significantly improves JSON result parsing and UI integration, since each Metric has a single or two results only.\\n\\nYou can check the list of new Me\", \"title\": \"Migration Guide\", \"description\": \"How to migrate to the new Evidently version?\", \"filename\": \"faq/migration.mdx\"}, {\"start\": 17000, \"content\": \"\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \\\"incorrect\\\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we're going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \\\"target\\\", and the LLM-judge response is the \\\"prediction\\\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\\\"label\\\",\\n        prediction_labels=\\\"Correctness\\\",\\n        pos_label = \\\"incorrect\\\")],\\n    categorical_columns=[\\\"label\\\", \\\"Correctness\\\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\\\"what we want to predict better\\\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let's use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let\\u2019s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don\\u2019t have a reference answer.\\n\\nHere's ho\", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}, {\"start\": 4000, \"content\": \"e(df),\\n    data_definition=DataDefinition(\\n        text_columns=[\\\"question\\\", \\\"answer\\\"]),\\n    descriptors=[\\n        Sentiment(\\\"answer\\\", alias=\\\"Sentiment\\\"),\\n        TextLength(\\\"answer\\\", alias=\\\"Length\\\"),\\n        IncludesWords(\\\"answer\\\", words_list=['sorry', 'apologize'], alias=\\\"Denials\\\"),\\n    ]\\n)\\n```\\n\\n**2. Aggregate results or run conditional checks**. Use these descriptors like any other dataset column when creating a Report. For example, here is how you summarize all descriptors and check that the text length is under 100 symbols.\\n\\n```python\\nreport = Report([\\n    TextEvals(),\\n    MaxValue(column=\\\"Length\\\", tests=[lt(100)]),\\n])\\n```\\n\\nThis decoupling means you can reuse descriptor outputs for multiple tests or aggregations without recomputation. It\\u2019s especially useful for LLM evaluations.\\n\\n<Card title=\\\"Descriptors\\\" href=\\\"/docs/library/descriptors\\\">\\n  Docs on adding descriptors.\\n</Card>\\n\\n### New Reports API\\n\\nAs you may have noticed in the example above, we made the changes to the core Report API. Here is how generating a Report with data summary preset for a single dataset works now:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    pd.DataFrame(source_df),\\n    data_definition=DataDefinition()\\n)\\n\\nreport = Report([\\n    DataSummaryPreset()\\n])\\n\\nmy_eval = report.run(eval_data, None)\\n```\\n\\nKey changes:\\n\\n- The Report object now defines the configuration (e.g., metrics to include).\\n- Running a Report returns a separate result object.\\n\\n<Card title=\\\"Reports\\\" href=\\\"/docs/library/report\\\">\\n  How to generate Reports.\\n</Card>\\n\\nAdditional improvement: you can also now use \\\"Group by\\\" to compute metrics for specific segments.\\n\\n### Test Suites joined with Reports\\n\\nMost importantly,  Reports and Tests are now unified. Previously, these were separate:\\n\\n- Reports provided an overview of metrics (e.g., distribution summaries, statistics).\\n- Tests verify pass/fail conditions (e.g., check for missing data or LLM quality thresholds).\\n\\nNow, the Test Suite mode is an optional extension of a Report. If \", \"title\": \"Migration Guide\", \"description\": \"How to migrate to the new Evidently version?\", \"filename\": \"faq/migration.mdx\"}, {\"start\": 2000, \"content\": \"notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\\\"OPENAI_API_KEY\\\"] = \\\"YOUR_KEY\\\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we'll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It's a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \\\"ground truth\\\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here's how you can create this dataset in one go:\\n\\n<Accordion title=\\\"Toy data to run the example\\\" defaultOpen={false}>\\n  ```python\\n  \", \"title\": \"LLM as a judge\", \"description\": \"How to create and evaluate an LLM judge.\", \"filename\": \"examples/LLM_judge.mdx\"}]\n",
      "</CONTEXT>\n",
      "To build an evaluation report using an LLM as a judge, you can follow these steps:\n",
      "\n",
      "1. **Installation and Imports**: First, make sure you have the Evidently library installed and import the necessary modules:\n",
      "\n",
      "   ```python\n",
      "   pip install evidently\n",
      "   ```\n",
      "\n",
      "   Then import the modules:\n",
      "\n",
      "   ```python\n",
      "   import pandas as pd\n",
      "   from evidently import Dataset, Report\n",
      "   from evidently.presets import TextEvals\n",
      "   ```\n",
      "\n",
      "2. **Setup OpenAI Key**: Set your OpenAI API key as an environment variable:\n",
      "\n",
      "   ```python\n",
      "   import os\n",
      "   os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"  # Replace with your actual key\n",
      "   ```\n",
      "\n",
      "3. **Create an Evaluation Dataset**: Prepare your dataset, which should include the necessary questions and responses, along with any manual labels as ground truth. For example:\n",
      "\n",
      "   ```python\n",
      "   eval_data = Dataset.from_pandas(\n",
      "       pd.DataFrame({\n",
      "           \"question\": [\"What is AI?\", \"Explain deep learning.\"],\n",
      "           \"response\": [\"Artificial Intelligence\", \"Deep learning is a subset of ML.\"],\n",
      "           \"correctness\": [\"correct\", \"incorrect\"]  # Your manual labels\n",
      "       }),\n",
      "       data_definition=DataDefinition()\n",
      "   )\n",
      "   ```\n",
      "\n",
      "4. **Run the LLM as a Judge**: Define the evaluation criteria, invoke your LLM, and run the report:\n",
      "\n",
      "   ```python\n",
      "   report = Report([\n",
      "       TextEvals()  # Evaluate the responses based on defined criteria\n",
      "   ])\n",
      "\n",
      "   my_eval = report.run(eval_data, None)\n",
      "   ```\n",
      "\n",
      "5. **Preview the Results**: You can preview the evaluation results in a local environment:\n",
      "\n",
      "   ```python\n",
      "   print(my_eval)  # This will give you a summary of the evaluation\n",
      "   ```\n",
      "\n",
      "6. **Upload to Evidently Cloud (Optional)**: If you are using Evidently Cloud, you can upload your results:\n",
      "\n",
      "   ```python\n",
      "   ws.add_run(project.id, my_eval, include_data=True)\n",
      "   ```\n",
      "\n",
      "This process will yield a report summarizing the evaluation made by the LLM. For further customization of the LLM and its evaluation criteria, refer to the detailed documentation available on the Evidently platform.\n",
      "\n",
      "References: \n",
      "- docs/library/report.mdx\n",
      "- examples/LLM_judge.mdx\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:27:22.609647Z",
     "start_time": "2025-10-25T20:27:22.608264Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "56e8418afa15e04d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
