{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-26T15:18:15.022537Z",
     "start_time": "2025-10-26T15:18:14.550990Z"
    }
   },
   "source": [
    "# Get Data from documents\n",
    "\n",
    "import requests\n",
    "from openai.types.responses import ResponseFunctionToolCall\n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for record in documents_raw:\n",
    "    course_name = record['course'] #data-engineering-zoomcamp#\n",
    "\n",
    "    for element in record['documents']: #documents[]\n",
    "        element['course'] = course_name\n",
    "        documents.append(element)"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:18:16.898005Z",
     "start_time": "2025-10-26T15:18:16.817774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create an Appendable Index ( Advantage is that we can keep adding to an appendable index)\n",
    "from minsearch import AppendableIndex\n",
    "\n",
    "\n",
    "# text fields are tokenized and put in an inverted index.\n",
    "# Overview of how the index workds below\n",
    "\"\"\"\n",
    "üß©Text Fields and  Inverted Index ‚Äî Complete Side-by-Side Diagram\n",
    "(for text_fields=[\"question\", \"text\", \"section\"])\n",
    "\n",
    "Example Documents:\n",
    "\n",
    "Doc 0:\n",
    "    question: \"What is deep learning?\"\n",
    "    text:     \"Deep learning is a subset of machine learning.\"\n",
    "    section:  \"AI Basics\"\n",
    "\n",
    "Doc 1:\n",
    "    question: \"How to use Python?\"\n",
    "    text:     \"Python is widely used for data analysis.\"\n",
    "    section:  \"Programming\"\n",
    "\n",
    "Doc 2:\n",
    "    question: \"What is machine learning?\"\n",
    "    text:     \"Machine learning involves algorithms that learn from data.\"\n",
    "    section:  \"AI Basics\"\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Tokenization (stop words removed, lowercase)\n",
    "\n",
    "Doc 0 tokens:\n",
    "    question ‚Üí [\"deep\", \"learning\"]\n",
    "    text     ‚Üí [\"deep\", \"learning\", \"subset\", \"machine\", \"learning\"]\n",
    "    section  ‚Üí [\"ai\", \"basics\"]\n",
    "\n",
    "Doc 1 tokens:\n",
    "    question ‚Üí [\"python\", \"use\"]\n",
    "    text     ‚Üí [\"python\", \"widely\", \"used\", \"data\", \"analysis\"]\n",
    "    section  ‚Üí [\"programming\"]\n",
    "\n",
    "Doc 2 tokens:\n",
    "    question ‚Üí [\"machine\", \"learning\"]\n",
    "    text     ‚Üí [\"machine\", \"learning\", \"involves\", \"algorithms\", \"learn\", \"data\"]\n",
    "    section  ‚Üí [\"ai\", \"basics\"]\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Inverted Index ‚Äî Side-by-Side View\n",
    "\n",
    "+----------------+---------------------------+------------------+\n",
    "| question field | text field                | section field    |\n",
    "+----------------+---------------------------+------------------+\n",
    "| deep     ‚Üí [0] | deep       ‚Üí [0]          | ai          ‚Üí [0,2] |\n",
    "| learning ‚Üí [0,2]| learning   ‚Üí [0,2]       | basics      ‚Üí [0,2] |\n",
    "| python   ‚Üí [1] | subset     ‚Üí [0]          | programming ‚Üí [1]   |\n",
    "| use      ‚Üí [1] | machine    ‚Üí [0,2]        |                  |\n",
    "| machine  ‚Üí [2] | python     ‚Üí [1]          |                  |\n",
    "|                | widely     ‚Üí [1]          |                  |\n",
    "|                | used       ‚Üí [1]          |                  |\n",
    "|                | data       ‚Üí [1,2]        |                  |\n",
    "|                | analysis   ‚Üí [1]          |                  |\n",
    "|                | involves   ‚Üí [2]          |                  |\n",
    "|                | algorithms ‚Üí [2]          |                  |\n",
    "|                | learn      ‚Üí [2]          |                  |\n",
    "+----------------+---------------------------+------------------+\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Example Search:\n",
    "\n",
    "Query: \"deep learning in AI\" ‚Üí [\"deep\", \"learning\", \"ai\"]\n",
    "\n",
    "Lookup in inverted index:\n",
    "\n",
    "question field:\n",
    "    \"deep\" ‚Üí [0]\n",
    "    \"learning\" ‚Üí [0,2]\n",
    "    \"ai\" ‚Üí []\n",
    "\n",
    "text field:\n",
    "    \"deep\" ‚Üí [0]\n",
    "    \"learning\" ‚Üí [0,2]\n",
    "    \"ai\" ‚Üí []\n",
    "\n",
    "section field:\n",
    "    \"deep\" ‚Üí []\n",
    "    \"learning\" ‚Üí []\n",
    "    \"ai\" ‚Üí [0,2]\n",
    "\n",
    "Candidate documents = union of all matches ‚Üí [0,2]\n",
    "\n",
    "Rank using TF-IDF across fields.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Summary:\n",
    "\n",
    "This diagram shows:\n",
    "\n",
    "1. Original documents\n",
    "2. Tokens extracted per text field\n",
    "3. Inverted index mapping tokens ‚Üí document IDs\n",
    "\n",
    "It demonstrates how `AppendableIndex` efficiently finds candidate documents\n",
    "without scanning all documents.\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "üß© Keyword Fields ‚Äî Side-by-Side Diagram\n",
    "(for keyword_fields=[\"category\", \"author\", \"year\"])\n",
    "\n",
    "Example Documents:\n",
    "\n",
    "Doc 0:\n",
    "    category: \"AI\"\n",
    "    author:   \"Andrew Ng\"\n",
    "    year:     2023\n",
    "\n",
    "Doc 1:\n",
    "    category: \"Programming\"\n",
    "    author:   \"Guido van Rossum\"\n",
    "    year:     2022\n",
    "\n",
    "Doc 2:\n",
    "    category: \"AI\"\n",
    "    author:   \"Geoff Hinton\"\n",
    "    year:     2023\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Keyword Field Mapping (Value ‚Üí Document IDs)\n",
    "\n",
    "+--------------+---------------------+------------+\n",
    "| category     | author              | year       |\n",
    "+--------------+---------------------+------------+\n",
    "| AI           ‚Üí [0,2]   | Andrew Ng        ‚Üí [0] | 2023 ‚Üí [0,2] |\n",
    "| Programming  ‚Üí [1]     | Guido van Rossum  ‚Üí [1] | 2022 ‚Üí [1]   |\n",
    "|              | Geoff Hinton      ‚Üí [2] |            |\n",
    "+--------------+---------------------+------------+\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Example Search:\n",
    "\n",
    "Query: \"learning\" with filters {\"category\": \"AI\", \"year\": 2023}\n",
    "\n",
    "1. Text search may match Docs 0, 1, 2 based on TF-IDF.\n",
    "2. Apply keyword filters:\n",
    "    - category = \"AI\" ‚Üí keep Docs [0,2]\n",
    "    - year = 2023      ‚Üí keep Docs [0,2]\n",
    "3. Result after filtering ‚Üí [Doc 0, Doc 2]\n",
    "4. Rank using TF-IDF across text fields.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Summary:\n",
    "\n",
    "- Keyword fields map exact values ‚Üí document IDs.\n",
    "- Enable filtering, faceted search, and grouping.\n",
    "- Complement text fields, which are scored for relevance.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# can\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)\n",
    "\n",
    "# to append , we can use the following function\n",
    "#index.append(XXXX);\n"
   ],
   "id": "a9b3482d6c55fbbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x1135c5c40>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:18:19.278608Z",
     "start_time": "2025-10-26T15:18:19.275591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create the search function to get results from Search Index create above\n",
    "\"\"\"\n",
    "üß© TF-IDF + Boost Explanation in AppendableIndex\n",
    "\n",
    "TF-IDF = Term Frequency √ó Inverse Document Frequency\n",
    "It measures how important a word is in a document relative to the entire corpus.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "1Ô∏è‚É£ Term Frequency (TF)\n",
    "- Counts how often a term appears in a document.\n",
    "- Sublinear scaling: TF = 1 + log(count), if count > 0\n",
    "- Example:\n",
    "    Doc 0, text: \"Deep learning learning learning\"\n",
    "    Count of \"learning\" = 3\n",
    "    TF(\"learning\") = 1 + log(3) ‚âà 2.10\n",
    "\n",
    "2Ô∏è‚É£ Inverse Document Frequency (IDF)\n",
    "- Measures how rare a term is across all documents.\n",
    "- Formula: IDF = log((N + 1) / (DF + 1)) + 1\n",
    "    - N = total number of documents\n",
    "    - DF = number of documents containing the term\n",
    "- Rare terms get higher IDF.\n",
    "\n",
    "Example:\n",
    "    3 documents contain \"learning\" in text field:\n",
    "        N = 3, DF(\"learning\") = 2\n",
    "        IDF(\"learning\") = log((3+1)/(2+1)) + 1 ‚âà 1.29\n",
    "\n",
    "3Ô∏è‚É£ TF-IDF\n",
    "- Multiply TF √ó IDF for each token in a document.\n",
    "- L2 normalize vectors to compare similarity with cosine similarity.\n",
    "\n",
    "Example Calculation for Doc 0:\n",
    "    - Token \"learning\": TF = 2.10, IDF = 1.29\n",
    "    - TF-IDF(\"learning\") = 2.10 * 1.29 ‚âà 2.71\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "4Ô∏è‚É£ Field Boosts\n",
    "\n",
    "- `boost_dict` in search allows weighting specific text fields.\n",
    "- Each field score is multiplied by its boost before combining.\n",
    "\n",
    "Example:\n",
    "\n",
    "Text fields: [\"question\", \"text\", \"section\"]\n",
    "Boosts: {\"question\": 2.0, \"text\": 1.0}  # section uses default 1.0\n",
    "\n",
    "Doc 0 raw TF-IDF scores for a query:\n",
    "+-----------+-----------+----------------+\n",
    "| Field     | Raw Score | Boosted Score  |\n",
    "+-----------+-----------+----------------+\n",
    "| question  | 0.5       | 0.5 * 2.0 = 1.0|\n",
    "| text      | 0.3       | 0.3 * 1.0 = 0.3|\n",
    "| section   | 0.2       | 0.2 * 1.0 = 0.2|\n",
    "+-----------+-----------+----------------+\n",
    "\n",
    "Total Score = 1.0 + 0.3 + 0.2 = 1.5\n",
    "- Boosting \"question\" doubles its impact on ranking.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "5Ô∏è‚É£ How Search Works with TF-IDF + Boost\n",
    "\n",
    "1. Tokenize query and documents.\n",
    "2. Compute TF-IDF vectors for query and each document (per text field).\n",
    "3. Apply L2 normalization.\n",
    "4. Calculate cosine similarity between query vector and document vectors.\n",
    "5. Multiply each field score by its boost (from `boost_dict`).\n",
    "6. Combine scores across fields for final ranking.\n",
    "7. Optionally, apply keyword filters to remove non-matching documents.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "Summary:\n",
    "\n",
    "- TF-IDF ranks documents based on query relevance.\n",
    "- Rare and query-specific terms get higher scores.\n",
    "- Field boosts allow tuning importance of specific fields.\n",
    "- Keyword fields filter results without affecting TF-IDF scoring.\n",
    "\"\"\"\n",
    "\n",
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'}, #This is used because we added it as a keyword field above\n",
    "        boost_dict=boost,\n",
    "        num_results=15,\n",
    "    )\n",
    "\n",
    "    return results"
   ],
   "id": "8731ab3b114caae5",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T14:56:40.992649Z",
     "start_time": "2025-10-26T14:56:40.981351Z"
    }
   },
   "cell_type": "code",
   "source": "len(search(\"how do I install kafka\"))",
   "id": "b06331766c5f77d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:18:29.374120Z",
     "start_time": "2025-10-26T15:18:29.371936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the search tool\n",
    "# This is needed because when the LLM is deciding to pick a \"tool\" it use the description and the properties to call the tool.\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": { #parameters\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n"
   ],
   "id": "616fd58c3754c541",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:18:35.725777Z",
     "start_time": "2025-10-26T15:18:35.491905Z"
    }
   },
   "cell_type": "code",
   "source": "!uv add toyaikit",
   "id": "197929cd61cfcbf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2mResolved \u001B[1m155 packages\u001B[0m \u001B[2min 11ms\u001B[0m\u001B[0m\r\n",
      "\u001B[2mAudited \u001B[1m136 packages\u001B[0m \u001B[2min 0.29ms\u001B[0m\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:18:37.830560Z",
     "start_time": "2025-10-26T15:18:37.828008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback\n",
    "from toyaikit.tools import Tools"
   ],
   "id": "c4742dcac94c275e",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T14:56:41.248955Z",
     "start_time": "2025-10-26T14:56:41.247437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set System prompt\n",
    "instructions = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call.\n",
    "Display the responses returned by the tool\n",
    "\"\"\".strip()"
   ],
   "id": "53a75e0b0528c2f9",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T14:56:41.253114Z",
     "start_time": "2025-10-26T14:56:41.251729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## USe the ToyAI kit helper function to map a tool to its schema.\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search,search_tool)"
   ],
   "id": "6704adc13c9e1334",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T14:56:41.271455Z",
     "start_time": "2025-10-26T14:56:41.256272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Create a chat\n",
    "\n",
    "chat_interface = IPythonChatInterface() #Creates an interface to show chat messages, function calls , reasoning\n",
    "\n",
    "# A runner has two loops, the Agentic Loop (loop function) and the Chat runner loop (run )\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=instructions,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ],
   "id": "c40803c12bbe07e6",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T14:57:03.539948Z",
     "start_time": "2025-10-26T14:56:41.274914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "callback = DisplayingRunnerCallback(chat_interface)\n",
    "\n",
    "question = 'how to fix Docker error with kafka'\n",
    "loop_result = runner.loop(prompt=question, callback=callback) #This it to only test the Agentic Loop"
   ],
   "id": "8076d3346e78a200",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To address your issue effectively, I'll look up common solutions for Docker errors related to Apache Kafka. Issues can vary widely, so it's important to identify possible causes such as configuration errors, connection issues, or compatibility problems.</p>\n",
       "<p>Let's find relevant solutions in the FAQ database.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"Docker error Kafka\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"Docker error Kafka\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_DoJjZPYq2gEQbvSasWLpHrOT', 'output': '[\\n  {\\n    \"text\": \"While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\\\n\\\\u2026\\\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077\\\\u2026\\\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\\\n\\\\u2026\\\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\\\n\\\\u2026\\\\nSolution:\\\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\\\nSolution 2:\\\\nCheck what Spark version your local machine has\\\\npyspark \\\\u2013version\\\\nspark-submit \\\\u2013version\\\\nAdd your version to SPARK_VERSION in build.sh\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"According to https://github.com/dpkp/kafka-python/\\\\n\\\\u201cDUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING\\\\u201d\\\\nUse pip install kafka-python-ng instead\",\\n    \"section\": \"Project\",\\n    \"question\": \"How to fix the error \\\\\"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\\\\"?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Run this command in terminal in the same directory (/docker/spark):\\\\nchmod +x build.sh\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: ./build.sh: Permission denied Error\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\"main\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\"java-kafka-rides\\\\\"\\\\narchiveClassifier = \\'\\'\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \\\\\"pip install kafka-python\\\\\", you can resolve the issue by using \\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\". If you have already installed kafka-python, you need to run \\\\\"pip uninstall kafka-python\\\\\" before executing \\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\" to resolve the compatibility issue.\\\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \\\\\"{{ env_var(\\'GCP_CREDENTIALS\\') }}\\\\\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\\\n\\\\u201cexport DBT_PROFILES_DBT=path/to/profiles.yml\\\\u201d\\\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\\\nOnce DIRs are set,:\\\\n\\\\u201cdbt debug \\\\u2013config-dir\\\\u201d\\\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\\\nThen create a trigger.py as such:\\\\nimport os\\\\nimport requests\\\\nclass MageTrigger:\\\\nOPTIONS = {\\\\n\\\\\"<pipeline_name>\\\\\": {\\\\n\\\\\"trigger_id\\\\\": 10,\\\\n\\\\\"key\\\\\": \\\\\"f3a1a4228fc64cfd85295b668c93f3b2\\\\\"\\\\n}\\\\n}\\\\n@staticmethod\\\\ndef trigger_pipeline(pipeline_name, variables=None):\\\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\\\\\"trigger_id\\\\\"]\\\\nkey = MageTrigger.OPTIONS[pipeline_name][\\\\\"key\\\\\"]\\\\nendpoint = f\\\\\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\\\\\"\\\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\\\npayload = {}\\\\nif variables is not None:\\\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\\\nreturn response\\\\nMageTrigger.trigger_pipeline(\\\\\"<pipeline_name>\\\\\")\\\\nFinally, after the mage server is up an running, simply this command:\\\\npython trigger.py from mage directory in terminal.\\\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\\\nYou can use this configuration in your DBT model:\\\\n{\\\\n\\\\\"field\\\\\": \\\\\"<field name>\\\\\",\\\\n\\\\\"data_type\\\\\": \\\\\"<timestamp | date | datetime | int64>\\\\\",\\\\n\\\\\"granularity\\\\\": \\\\\"<hour | day | month | year>\\\\\"\\\\n# Only required if data_type is \\\\\"int64\\\\\"\\\\n\\\\\"range\\\\\": {\\\\n\\\\\"start\\\\\": <int>,\\\\n\\\\\"end\\\\\": <int>,\\\\n\\\\\"interval\\\\\": <int>\\\\n}\\\\n}\\\\nand for clustering\\\\n{{\\\\nconfig(\\\\nmaterialized = \\\\\"table\\\\\",\\\\ncluster_by = \\\\\"order_id\\\\\",\\\\n)\\\\n}}\\\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs\",\\n    \"section\": \"Triggers in Mage via CLI\",\\n    \"question\": \"Encountering the error \\\\\"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\\\\" when running \\\\\"from kafka import KafkaProducer\\\\\" in Jupyter Notebook for Module 6 Homework?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Start a new terminal\\\\nRun: docker ps\\\\nCopy the CONTAINER ID of the spark-master container\\\\nRun: docker exec -it <spark_master_container_id> bash\\\\nRun: cat logs/spark-master.out\\\\nCheck for the log when the error happened\\\\nGoogle the error message from there\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Ans: [source] Yes, it is so that we can observe the changes as we\\\\u2019re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.\",\\n    \"section\": \"Workshop 2 - RisingWave\",\\n    \"question\": \"stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\\\nUse the git bash terminal in windows.\\\\nActivate python venv from git bash: source .venv/Scripts/activate\\\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\\\nNow from git bash, run the seed-kafka cmd. It should work now.\\\\nAdditional Notes:\\\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\\\nThe equivalent of source commands.sh  in Powershell is . .\\\\\\\\commands.sh from the workshop directory.\\\\nHope this can save you from some trouble in case you\\'re doing this workshop on Windows like I am.\\\\n\\\\u2014--------------------------------------------------------------------------------------\",\\n    \"section\": \"Workshop 2 - RisingWave\",\\n    \"question\": \"Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Restarting all services worked for me:\\\\ndocker-compose down\\\\ndocker-compose up\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Python Kafka: \\\\u2018KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\\\u2019 when running stream-example/producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\\\nTo create a virtual env and install packages (run only once)\\\\npython -m venv env\\\\nsource env/bin/activate\\\\npip install -r ../requirements.txt\\\\nTo activate it (you\\'ll need to run it every time you need the virtual env):\\\\nsource env/bin/activate\\\\nTo deactivate it:\\\\ndeactivate\\\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it\\'s env/Scripts/activate)\\\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Module \\\\u201ckafka\\\\u201d not found when trying to run producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If you get this error, know that you have not built your sparks and juypter images. This images aren\\\\u2019t readily available on dockerHub.\\\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require \\'docker login\\': denied: requested access to the resource is denied\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here are some potential solutions for resolving Docker errors related to Kafka:</p>\n",
       "<ol>\n",
       "<li><p><strong>Check Kafka Broker Status</strong>:</p>\n",
       "<ul>\n",
       "<li>If you encounter the error indicating no brokers are available (<code>kafka.errors.NoBrokersAvailable</code>), first confirm that your Kafka broker container is running. Use the command:<pre><code class=\"language-bash\">docker ps\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>If it's not running, navigate to your Docker Compose YAML file directory and start all instances with:<pre><code class=\"language-bash\">docker-compose up -d\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Common Timeout Error</strong>:</p>\n",
       "<ul>\n",
       "<li>For the <code>KafkaTimeoutError: Failed to update metadata after 60.0 secs</code> error, restarting all services can help:<pre><code class=\"language-bash\">docker-compose down\n",
       "docker-compose up\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Resolving Connection Failures</strong>:</p>\n",
       "<ul>\n",
       "<li>If you're experiencing connection issues with Spark and Kafka (for example, when using <code>spark-submit</code>), verify that your Spark master is running. You can check the logs in the terminal:<pre><code class=\"language-bash\">docker exec -it &lt;spark_master_container_id&gt; bash\n",
       "cat logs/spark-master.out\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Version Compatibility</strong>:</p>\n",
       "<ul>\n",
       "<li>Ensure that the versions of the Kafka libraries you're using in your development environment are compatible. Sometimes downgrading or updating the versions helps resolve dependency issues.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Environment Setup</strong>:</p>\n",
       "<ul>\n",
       "<li>Always ensure you are running your scripts in the correct environment. Setting up a Python virtual environment might help isolate dependencies:<pre><code class=\"language-bash\">python -m venv env\n",
       "source env/bin/activate\n",
       "pip install -r requirements.txt\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you provide more specific details about the error you're encountering, I can help with tailored solutions!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T14:57:03.570389Z",
     "start_time": "2025-10-26T14:57:03.564841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "def prettyprint_json(obj):\n",
    "    \"\"\"\n",
    "    Pretty-prints a nested object (like a LoopResult) as JSON.\n",
    "    Automatically converts dataclass-like objects to dicts if needed.\n",
    "    \"\"\"\n",
    "    def to_serializable(o):\n",
    "        # Convert dataclass-like or custom objects to dictionaries\n",
    "        if hasattr(o, \"__dict__\"):\n",
    "            return {k: to_serializable(v) for k, v in vars(o).items()}\n",
    "        elif isinstance(o, list):\n",
    "            return [to_serializable(i) for i in o]\n",
    "        elif isinstance(o, dict):\n",
    "            return {k: to_serializable(v) for k, v in o.items()}\n",
    "        else:\n",
    "            return o\n",
    "\n",
    "    # Convert and pretty-print\n",
    "    print(json.dumps(to_serializable(obj), indent=2, ensure_ascii=False))"
   ],
   "id": "10f7270e7068f551",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T14:57:03.587707Z",
     "start_time": "2025-10-26T14:57:03.584196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "prettyprint_json(loop_result)"
   ],
   "id": "57a64571f8dbb3dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"new_messages\": [\n",
      "    {\n",
      "      \"role\": \"developer\",\n",
      "      \"content\": \"You're a course teaching assistant.\\nYou're given a question from a course student and your task is to answer it.\\n\\nIf you want to look up the answer, explain why before making the call.\\nDisplay the responses returned by the tool\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"how to fix Docker error with kafka\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_0fc17b85f430f4eb0068fe36ace560819597b8ed9c18245b74\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"To address your issue effectively, I'll look up common solutions for Docker errors related to Apache Kafka. Issues can vary widely, so it's important to identify possible causes such as configuration errors, connection issues, or compatibility problems.\\n\\nLet's find relevant solutions in the FAQ database.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"arguments\": \"{\\\"query\\\":\\\"Docker error Kafka\\\"}\",\n",
      "      \"call_id\": \"call_DoJjZPYq2gEQbvSasWLpHrOT\",\n",
      "      \"name\": \"search\",\n",
      "      \"type\": \"function_call\",\n",
      "      \"id\": \"fc_0fc17b85f430f4eb0068fe36b2614c8195a58d8103fd17c380\",\n",
      "      \"status\": \"completed\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function_call_output\",\n",
      "      \"call_id\": \"call_DoJjZPYq2gEQbvSasWLpHrOT\",\n",
      "      \"output\": \"[\\n  {\\n    \\\"text\\\": \\\"While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\\\n\\\\u2026\\\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077\\\\u2026\\\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\\\n\\\\u2026\\\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\\\n\\\\u2026\\\\nSolution:\\\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\\\nSolution 2:\\\\nCheck what Spark version your local machine has\\\\npyspark \\\\u2013version\\\\nspark-submit \\\\u2013version\\\\nAdd your version to SPARK_VERSION in build.sh\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"According to https://github.com/dpkp/kafka-python/\\\\n\\\\u201cDUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING\\\\u201d\\\\nUse pip install kafka-python-ng instead\\\",\\n    \\\"section\\\": \\\"Project\\\",\\n    \\\"question\\\": \\\"How to fix the error \\\\\\\"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\\\\\\\"?\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Run this command in terminal in the same directory (/docker/spark):\\\\nchmod +x build.sh\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: ./build.sh: Permission denied Error\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Kafka- python videos have low audio and hard to follow up\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\\\"main\\\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\\\"java-kafka-rides\\\\\\\"\\\\narchiveClassifier = ''\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \\\\\\\"pip install kafka-python\\\\\\\", you can resolve the issue by using \\\\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\\\". If you have already installed kafka-python, you need to run \\\\\\\"pip uninstall kafka-python\\\\\\\" before executing \\\\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\\\" to resolve the compatibility issue.\\\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \\\\\\\"{{ env_var('GCP_CREDENTIALS') }}\\\\\\\". The GCP_CREDENTIALS variable holds the full path to the service account key's JSON file. Adding the following line within the failed code block resolved the issue: os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.environ.get('GCP_CREDENTIALS').\\\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\\\n\\\\u201cexport DBT_PROFILES_DBT=path/to/profiles.yml\\\\u201d\\\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\\\nOnce DIRs are set,:\\\\n\\\\u201cdbt debug \\\\u2013config-dir\\\\u201d\\\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\\\nThen create a trigger.py as such:\\\\nimport os\\\\nimport requests\\\\nclass MageTrigger:\\\\nOPTIONS = {\\\\n\\\\\\\"<pipeline_name>\\\\\\\": {\\\\n\\\\\\\"trigger_id\\\\\\\": 10,\\\\n\\\\\\\"key\\\\\\\": \\\\\\\"f3a1a4228fc64cfd85295b668c93f3b2\\\\\\\"\\\\n}\\\\n}\\\\n@staticmethod\\\\ndef trigger_pipeline(pipeline_name, variables=None):\\\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\\\\\\\"trigger_id\\\\\\\"]\\\\nkey = MageTrigger.OPTIONS[pipeline_name][\\\\\\\"key\\\\\\\"]\\\\nendpoint = f\\\\\\\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\\\\\\\"\\\\nheaders = {'Content-Type': 'application/json'}\\\\npayload = {}\\\\nif variables is not None:\\\\npayload['pipeline_run'] = {'variables': variables}\\\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\\\nreturn response\\\\nMageTrigger.trigger_pipeline(\\\\\\\"<pipeline_name>\\\\\\\")\\\\nFinally, after the mage server is up an running, simply this command:\\\\npython trigger.py from mage directory in terminal.\\\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\\\nYou can use this configuration in your DBT model:\\\\n{\\\\n\\\\\\\"field\\\\\\\": \\\\\\\"<field name>\\\\\\\",\\\\n\\\\\\\"data_type\\\\\\\": \\\\\\\"<timestamp | date | datetime | int64>\\\\\\\",\\\\n\\\\\\\"granularity\\\\\\\": \\\\\\\"<hour | day | month | year>\\\\\\\"\\\\n# Only required if data_type is \\\\\\\"int64\\\\\\\"\\\\n\\\\\\\"range\\\\\\\": {\\\\n\\\\\\\"start\\\\\\\": <int>,\\\\n\\\\\\\"end\\\\\\\": <int>,\\\\n\\\\\\\"interval\\\\\\\": <int>\\\\n}\\\\n}\\\\nand for clustering\\\\n{{\\\\nconfig(\\\\nmaterialized = \\\\\\\"table\\\\\\\",\\\\ncluster_by = \\\\\\\"order_id\\\\\\\",\\\\n)\\\\n}}\\\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs\\\",\\n    \\\"section\\\": \\\"Triggers in Mage via CLI\\\",\\n    \\\"question\\\": \\\"Encountering the error \\\\\\\"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\\\\\\\" when running \\\\\\\"from kafka import KafkaProducer\\\\\\\" in Jupyter Notebook for Module 6 Homework?\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Start a new terminal\\\\nRun: docker ps\\\\nCopy the CONTAINER ID of the spark-master container\\\\nRun: docker exec -it <spark_master_container_id> bash\\\\nRun: cat logs/spark-master.out\\\\nCheck for the log when the error happened\\\\nGoogle the error message from there\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Ans: [source] Yes, it is so that we can observe the changes as we\\\\u2019re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.\\\",\\n    \\\"section\\\": \\\"Workshop 2 - RisingWave\\\",\\n    \\\"question\\\": \\\"stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\\\nUse the git bash terminal in windows.\\\\nActivate python venv from git bash: source .venv/Scripts/activate\\\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\\\nNow from git bash, run the seed-kafka cmd. It should work now.\\\\nAdditional Notes:\\\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\\\nThe equivalent of source commands.sh  in Powershell is . .\\\\\\\\commands.sh from the workshop directory.\\\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\\\n\\\\u2014--------------------------------------------------------------------------------------\\\",\\n    \\\"section\\\": \\\"Workshop 2 - RisingWave\\\",\\n    \\\"question\\\": \\\"Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Restarting all services worked for me:\\\\ndocker-compose down\\\\ndocker-compose up\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: \\\\u2018KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\\\u2019 when running stream-example/producer.py\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\\\nTo create a virtual env and install packages (run only once)\\\\npython -m venv env\\\\nsource env/bin/activate\\\\npip install -r ../requirements.txt\\\\nTo activate it (you'll need to run it every time you need the virtual env):\\\\nsource env/bin/activate\\\\nTo deactivate it:\\\\ndeactivate\\\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Module \\\\u201ckafka\\\\u201d not found when trying to run producer.py\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"If you get this error, know that you have not built your sparks and juypter images. This images aren\\\\u2019t readily available on dockerHub.\\\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  }\\n]\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_0fc17b85f430f4eb0068fe36b92c0c8195ad2f915c8a42299e\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"Here are some potential solutions for resolving Docker errors related to Kafka:\\n\\n1. **Check Kafka Broker Status**: \\n   - If you encounter the error indicating no brokers are available (`kafka.errors.NoBrokersAvailable`), first confirm that your Kafka broker container is running. Use the command:\\n     ```bash\\n     docker ps\\n     ```\\n   - If it's not running, navigate to your Docker Compose YAML file directory and start all instances with:\\n     ```bash\\n     docker-compose up -d\\n     ```\\n\\n2. **Common Timeout Error**: \\n   - For the `KafkaTimeoutError: Failed to update metadata after 60.0 secs` error, restarting all services can help:\\n     ```bash\\n     docker-compose down\\n     docker-compose up\\n     ```\\n\\n3. **Resolving Connection Failures**: \\n   - If you're experiencing connection issues with Spark and Kafka (for example, when using `spark-submit`), verify that your Spark master is running. You can check the logs in the terminal:\\n     ```bash\\n     docker exec -it <spark_master_container_id> bash\\n     cat logs/spark-master.out\\n     ```\\n\\n4. **Version Compatibility**: \\n   - Ensure that the versions of the Kafka libraries you're using in your development environment are compatible. Sometimes downgrading or updating the versions helps resolve dependency issues.\\n\\n5. **Environment Setup**: \\n   - Always ensure you are running your scripts in the correct environment. Setting up a Python virtual environment might help isolate dependencies:\\n     ```bash\\n     python -m venv env\\n     source env/bin/activate\\n     pip install -r requirements.txt\\n     ```\\n\\nIf you provide more specific details about the error you're encountering, I can help with tailored solutions!\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"all_messages\": [\n",
      "    {\n",
      "      \"role\": \"developer\",\n",
      "      \"content\": \"You're a course teaching assistant.\\nYou're given a question from a course student and your task is to answer it.\\n\\nIf you want to look up the answer, explain why before making the call.\\nDisplay the responses returned by the tool\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"how to fix Docker error with kafka\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_0fc17b85f430f4eb0068fe36ace560819597b8ed9c18245b74\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"To address your issue effectively, I'll look up common solutions for Docker errors related to Apache Kafka. Issues can vary widely, so it's important to identify possible causes such as configuration errors, connection issues, or compatibility problems.\\n\\nLet's find relevant solutions in the FAQ database.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    },\n",
      "    {\n",
      "      \"arguments\": \"{\\\"query\\\":\\\"Docker error Kafka\\\"}\",\n",
      "      \"call_id\": \"call_DoJjZPYq2gEQbvSasWLpHrOT\",\n",
      "      \"name\": \"search\",\n",
      "      \"type\": \"function_call\",\n",
      "      \"id\": \"fc_0fc17b85f430f4eb0068fe36b2614c8195a58d8103fd17c380\",\n",
      "      \"status\": \"completed\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"function_call_output\",\n",
      "      \"call_id\": \"call_DoJjZPYq2gEQbvSasWLpHrOT\",\n",
      "      \"output\": \"[\\n  {\\n    \\\"text\\\": \\\"While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\\\n\\\\u2026\\\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077\\\\u2026\\\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\\\n\\\\u2026\\\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\\\n\\\\u2026\\\\nSolution:\\\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\\\nSolution 2:\\\\nCheck what Spark version your local machine has\\\\npyspark \\\\u2013version\\\\nspark-submit \\\\u2013version\\\\nAdd your version to SPARK_VERSION in build.sh\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"If you have this error, it most likely that your kafka broker docker container is not working.\\\\nUse docker ps to confirm\\\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"According to https://github.com/dpkp/kafka-python/\\\\n\\\\u201cDUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING\\\\u201d\\\\nUse pip install kafka-python-ng instead\\\",\\n    \\\"section\\\": \\\"Project\\\",\\n    \\\"question\\\": \\\"How to fix the error \\\\\\\"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\\\\\\\"?\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Run this command in terminal in the same directory (/docker/spark):\\\\nchmod +x build.sh\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: ./build.sh: Permission denied Error\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\\\nKafka Python Videos - Rides.csv\\\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Kafka- python videos have low audio and hard to follow up\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"For example, when running JsonConsumer.java, got:\\\\nConsuming form kafka started\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nRESULTS:::0\\\\nOr when running JsonProducer.java, got:\\\\nException in thread \\\\\\\"main\\\\\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\\\nSolution:\\\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\\\nfastavro: pip install fastavro\\\\nAbhirup Ghosh\\\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\\\nSolution:\\\\nIn build.gradle file, I added the following at the end:\\\\nshadowJar {\\\\narchiveBaseName = \\\\\\\"java-kafka-rides\\\\\\\"\\\\narchiveClassifier = ''\\\\n}\\\\nAnd then in the command line ran \\\\u2018gradle shadowjar\\\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \\\\\\\"pip install kafka-python\\\\\\\", you can resolve the issue by using \\\\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\\\". If you have already installed kafka-python, you need to run \\\\\\\"pip uninstall kafka-python\\\\\\\" before executing \\\\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\\\" to resolve the compatibility issue.\\\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \\\\\\\"{{ env_var('GCP_CREDENTIALS') }}\\\\\\\". The GCP_CREDENTIALS variable holds the full path to the service account key's JSON file. Adding the following line within the failed code block resolved the issue: os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.environ.get('GCP_CREDENTIALS').\\\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\\\n\\\\u201cexport DBT_PROFILES_DBT=path/to/profiles.yml\\\\u201d\\\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\\\nOnce DIRs are set,:\\\\n\\\\u201cdbt debug \\\\u2013config-dir\\\\u201d\\\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\\\nThen create a trigger.py as such:\\\\nimport os\\\\nimport requests\\\\nclass MageTrigger:\\\\nOPTIONS = {\\\\n\\\\\\\"<pipeline_name>\\\\\\\": {\\\\n\\\\\\\"trigger_id\\\\\\\": 10,\\\\n\\\\\\\"key\\\\\\\": \\\\\\\"f3a1a4228fc64cfd85295b668c93f3b2\\\\\\\"\\\\n}\\\\n}\\\\n@staticmethod\\\\ndef trigger_pipeline(pipeline_name, variables=None):\\\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\\\\\\\"trigger_id\\\\\\\"]\\\\nkey = MageTrigger.OPTIONS[pipeline_name][\\\\\\\"key\\\\\\\"]\\\\nendpoint = f\\\\\\\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\\\\\\\"\\\\nheaders = {'Content-Type': 'application/json'}\\\\npayload = {}\\\\nif variables is not None:\\\\npayload['pipeline_run'] = {'variables': variables}\\\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\\\nreturn response\\\\nMageTrigger.trigger_pipeline(\\\\\\\"<pipeline_name>\\\\\\\")\\\\nFinally, after the mage server is up an running, simply this command:\\\\npython trigger.py from mage directory in terminal.\\\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\\\nYou can use this configuration in your DBT model:\\\\n{\\\\n\\\\\\\"field\\\\\\\": \\\\\\\"<field name>\\\\\\\",\\\\n\\\\\\\"data_type\\\\\\\": \\\\\\\"<timestamp | date | datetime | int64>\\\\\\\",\\\\n\\\\\\\"granularity\\\\\\\": \\\\\\\"<hour | day | month | year>\\\\\\\"\\\\n# Only required if data_type is \\\\\\\"int64\\\\\\\"\\\\n\\\\\\\"range\\\\\\\": {\\\\n\\\\\\\"start\\\\\\\": <int>,\\\\n\\\\\\\"end\\\\\\\": <int>,\\\\n\\\\\\\"interval\\\\\\\": <int>\\\\n}\\\\n}\\\\nand for clustering\\\\n{{\\\\nconfig(\\\\nmaterialized = \\\\\\\"table\\\\\\\",\\\\ncluster_by = \\\\\\\"order_id\\\\\\\",\\\\n)\\\\n}}\\\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs\\\",\\n    \\\"section\\\": \\\"Triggers in Mage via CLI\\\",\\n    \\\"question\\\": \\\"Encountering the error \\\\\\\"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'\\\\\\\" when running \\\\\\\"from kafka import KafkaProducer\\\\\\\" in Jupyter Notebook for Module 6 Homework?\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Start a new terminal\\\\nRun: docker ps\\\\nCopy the CONTAINER ID of the spark-master container\\\\nRun: docker exec -it <spark_master_container_id> bash\\\\nRun: cat logs/spark-master.out\\\\nCheck for the log when the error happened\\\\nGoogle the error message from there\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Ans: [source] Yes, it is so that we can observe the changes as we\\\\u2019re working on the queries in real-time. The script is changing the date timestamp to the current time, so our queries with the now()filter would work. Open another terminal tab to copy+paste the queries while the stream-kafka script is running in the background.\\\\nNoel: I have recently increased this up to 100 at a time, you may pull the latest changes from the repository.\\\",\\n    \\\"section\\\": \\\"Workshop 2 - RisingWave\\\",\\n    \\\"question\\\": \\\"stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\\\nUse the git bash terminal in windows.\\\\nActivate python venv from git bash: source .venv/Scripts/activate\\\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\\\nNow from git bash, run the seed-kafka cmd. It should work now.\\\\nAdditional Notes:\\\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\\\nThe equivalent of source commands.sh  in Powershell is . .\\\\\\\\commands.sh from the workshop directory.\\\\nHope this can save you from some trouble in case you're doing this workshop on Windows like I am.\\\\n\\\\u2014--------------------------------------------------------------------------------------\\\",\\n    \\\"section\\\": \\\"Workshop 2 - RisingWave\\\",\\n    \\\"question\\\": \\\"Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Restarting all services worked for me:\\\\ndocker-compose down\\\\ndocker-compose up\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Python Kafka: \\\\u2018KafkaTimeoutError: Failed to update metadata after 60.0 secs.\\\\u2019 when running stream-example/producer.py\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\\\nTo create a virtual env and install packages (run only once)\\\\npython -m venv env\\\\nsource env/bin/activate\\\\npip install -r ../requirements.txt\\\\nTo activate it (you'll need to run it every time you need the virtual env):\\\\nsource env/bin/activate\\\\nTo deactivate it:\\\\ndeactivate\\\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"Module \\\\u201ckafka\\\\u201d not found when trying to run producer.py\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  },\\n  {\\n    \\\"text\\\": \\\"If you get this error, know that you have not built your sparks and juypter images. This images aren\\\\u2019t readily available on dockerHub.\\\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose\\\",\\n    \\\"section\\\": \\\"Module 6: streaming with kafka\\\",\\n    \\\"question\\\": \\\"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\\\",\\n    \\\"course\\\": \\\"data-engineering-zoomcamp\\\"\\n  }\\n]\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"msg_0fc17b85f430f4eb0068fe36b92c0c8195ad2f915c8a42299e\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"Here are some potential solutions for resolving Docker errors related to Kafka:\\n\\n1. **Check Kafka Broker Status**: \\n   - If you encounter the error indicating no brokers are available (`kafka.errors.NoBrokersAvailable`), first confirm that your Kafka broker container is running. Use the command:\\n     ```bash\\n     docker ps\\n     ```\\n   - If it's not running, navigate to your Docker Compose YAML file directory and start all instances with:\\n     ```bash\\n     docker-compose up -d\\n     ```\\n\\n2. **Common Timeout Error**: \\n   - For the `KafkaTimeoutError: Failed to update metadata after 60.0 secs` error, restarting all services can help:\\n     ```bash\\n     docker-compose down\\n     docker-compose up\\n     ```\\n\\n3. **Resolving Connection Failures**: \\n   - If you're experiencing connection issues with Spark and Kafka (for example, when using `spark-submit`), verify that your Spark master is running. You can check the logs in the terminal:\\n     ```bash\\n     docker exec -it <spark_master_container_id> bash\\n     cat logs/spark-master.out\\n     ```\\n\\n4. **Version Compatibility**: \\n   - Ensure that the versions of the Kafka libraries you're using in your development environment are compatible. Sometimes downgrading or updating the versions helps resolve dependency issues.\\n\\n5. **Environment Setup**: \\n   - Always ensure you are running your scripts in the correct environment. Setting up a Python virtual environment might help isolate dependencies:\\n     ```bash\\n     python -m venv env\\n     source env/bin/activate\\n     pip install -r requirements.txt\\n     ```\\n\\nIf you provide more specific details about the error you're encountering, I can help with tailored solutions!\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"tokens\": {\n",
      "    \"input_tokens\": 3828,\n",
      "    \"output_tokens\": 431,\n",
      "    \"total_tokens\": 4259\n",
      "  },\n",
      "  \"cost\": {\n",
      "    \"input_cost\": 0.0005742,\n",
      "    \"output_cost\": 0.0002586,\n",
      "    \"total_cost\": 0.0008328\n",
      "  },\n",
      "  \"last_message\": \"Here are some potential solutions for resolving Docker errors related to Kafka:\\n\\n1. **Check Kafka Broker Status**: \\n   - If you encounter the error indicating no brokers are available (`kafka.errors.NoBrokersAvailable`), first confirm that your Kafka broker container is running. Use the command:\\n     ```bash\\n     docker ps\\n     ```\\n   - If it's not running, navigate to your Docker Compose YAML file directory and start all instances with:\\n     ```bash\\n     docker-compose up -d\\n     ```\\n\\n2. **Common Timeout Error**: \\n   - For the `KafkaTimeoutError: Failed to update metadata after 60.0 secs` error, restarting all services can help:\\n     ```bash\\n     docker-compose down\\n     docker-compose up\\n     ```\\n\\n3. **Resolving Connection Failures**: \\n   - If you're experiencing connection issues with Spark and Kafka (for example, when using `spark-submit`), verify that your Spark master is running. You can check the logs in the terminal:\\n     ```bash\\n     docker exec -it <spark_master_container_id> bash\\n     cat logs/spark-master.out\\n     ```\\n\\n4. **Version Compatibility**: \\n   - Ensure that the versions of the Kafka libraries you're using in your development environment are compatible. Sometimes downgrading or updating the versions helps resolve dependency issues.\\n\\n5. **Environment Setup**: \\n   - Always ensure you are running your scripts in the correct environment. Setting up a Python virtual environment might help isolate dependencies:\\n     ```bash\\n     python -m venv env\\n     source env/bin/activate\\n     pip install -r requirements.txt\\n     ```\\n\\nIf you provide more specific details about the error you're encountering, I can help with tailored solutions!\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:05:46.926456Z",
     "start_time": "2025-10-26T14:57:03.594260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Run activates chat loop\n",
    "\n",
    "runner.run()"
   ],
   "id": "c4183dae4b57984f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"Module 1 success tips\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"Module 1 success tips\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_k1TLlzVSywiSmnOR92njEQnW', 'output': '[\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Error raised during the jupyter notebook\\\\u2019s cell execution:\\\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\').\\\\nSolution: Need to install Python module \\\\u201cpsycopg2\\\\u201d. Can be installed by Conda or pip.\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named \\'psycopg2\\'.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\\\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing\",\\n    \"section\": \"Module 2: Workflow Orchestration\",\\n    \"question\": \"Where are the FAQ questions from the previous cohorts for the orchestration module?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If below does not work, then download the latest available py4j version with\\\\nconda install -c conda-forge py4j\\\\nTake care of the latest version number in the website to replace appropriately.\\\\nNow add\\\\nexport PYTHONPATH=\\\\\"${SPARK_HOME}/python/:$PYTHONPATH\\\\\"\\\\nexport PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\\\\\"\\\\nin your  .bashrc file.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4J Error - ModuleNotFoundError: No module named \\'py4j\\' (Solve with latest version)\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"AttributeError: \\'DataFrame\\' object has no attribute \\'iteritems\\'\\\\nthis is because the method inside the pyspark refers to a package that has been already deprecated\\\\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\\\nYou can do this code below, which is mentioned in the stackoverflow link above:\\\\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn\\'t work. What can I do?\\\\nError\\\\nInsufficient \\'SSD_TOTAL_GB\\' quota. Requested 500.0, available 250.0.\\\\nRequest ID: 17942272465025572271\\\\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\\\nMaster Node:\\\\nMachine type: n2-standard-2\\\\nPrimary disk size: 85 GB\\\\nWorker Node:\\\\nNumber of worker nodes: 2\\\\nMachine type: n2-standard-2\\\\nPrimary disk size: 80 GB\\\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\\\nTo create a virtual env and install packages (run only once)\\\\npython -m venv env\\\\nsource env/bin/activate\\\\npip install -r ../requirements.txt\\\\nTo activate it (you\\'ll need to run it every time you need the virtual env):\\\\nsource env/bin/activate\\\\nTo deactivate it:\\\\ndeactivate\\\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it\\'s env/Scripts/activate)\\\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Module \\\\u201ckafka\\\\u201d not found when trying to run producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"\\\\u2705SOLUTION: pip install confluent-kafka[avro].\\\\nFor some reason, Conda also doesn\\'t include this when installing confluent-kafka via pip.\\\\nMore sources on Anaconda and confluent-kafka issues:\\\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"ModuleNotFoundError: No module named \\'avro\\'\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \\\\\"pip install kafka-python\\\\\", you can resolve the issue by using \\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\". If you have already installed kafka-python, you need to run \\\\\"pip uninstall kafka-python\\\\\" before executing \\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\" to resolve the compatibility issue.\\\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \\\\\"{{ env_var(\\'GCP_CREDENTIALS\\') }}\\\\\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\\\n\\\\u201cexport DBT_PROFILES_DBT=path/to/profiles.yml\\\\u201d\\\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\\\nOnce DIRs are set,:\\\\n\\\\u201cdbt debug \\\\u2013config-dir\\\\u201d\\\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\\\nThen create a trigger.py as such:\\\\nimport os\\\\nimport requests\\\\nclass MageTrigger:\\\\nOPTIONS = {\\\\n\\\\\"<pipeline_name>\\\\\": {\\\\n\\\\\"trigger_id\\\\\": 10,\\\\n\\\\\"key\\\\\": \\\\\"f3a1a4228fc64cfd85295b668c93f3b2\\\\\"\\\\n}\\\\n}\\\\n@staticmethod\\\\ndef trigger_pipeline(pipeline_name, variables=None):\\\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\\\\\"trigger_id\\\\\"]\\\\nkey = MageTrigger.OPTIONS[pipeline_name][\\\\\"key\\\\\"]\\\\nendpoint = f\\\\\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\\\\\"\\\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\\\npayload = {}\\\\nif variables is not None:\\\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\\\nreturn response\\\\nMageTrigger.trigger_pipeline(\\\\\"<pipeline_name>\\\\\")\\\\nFinally, after the mage server is up an running, simply this command:\\\\npython trigger.py from mage directory in terminal.\\\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\\\nYou can use this configuration in your DBT model:\\\\n{\\\\n\\\\\"field\\\\\": \\\\\"<field name>\\\\\",\\\\n\\\\\"data_type\\\\\": \\\\\"<timestamp | date | datetime | int64>\\\\\",\\\\n\\\\\"granularity\\\\\": \\\\\"<hour | day | month | year>\\\\\"\\\\n# Only required if data_type is \\\\\"int64\\\\\"\\\\n\\\\\"range\\\\\": {\\\\n\\\\\"start\\\\\": <int>,\\\\n\\\\\"end\\\\\": <int>,\\\\n\\\\\"interval\\\\\": <int>\\\\n}\\\\n}\\\\nand for clustering\\\\n{{\\\\nconfig(\\\\nmaterialized = \\\\\"table\\\\\",\\\\ncluster_by = \\\\\"order_id\\\\\",\\\\n)\\\\n}}\\\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs\",\\n    \"section\": \"Triggers in Mage via CLI\",\\n    \"question\": \"Encountering the error \\\\\"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\\\\" when running \\\\\"from kafka import KafkaProducer\\\\\" in Jupyter Notebook for Module 6 Homework?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"According to https://github.com/dpkp/kafka-python/\\\\n\\\\u201cDUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING\\\\u201d\\\\nUse pip install kafka-python-ng instead\",\\n    \"section\": \"Project\",\\n    \"question\": \"How to fix the error \\\\\"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\\\\"?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Many people aren\\\\u2019t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.\",\\n    \"section\": \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE.\",\\n    \"question\": \"Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"You can get the version of py4j from inside docker using this command\\\\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \\\\\"ls /opt/spark/python/lib\\\\\"\",\\n    \"section\": \"Project\",\\n    \"question\": \"Spark docker - `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To excel in Module 1, which focuses on Docker and Terraform, you should keep these tips in mind:</p>\n",
       "<ol>\n",
       "<li><p><strong>Understand Docker Basics</strong>: Familiarize yourself with Docker concepts such as containers, images, and Dockerfiles. This foundation is crucial for managing applications in isolated environments.</p>\n",
       "</li>\n",
       "<li><p><strong>Use Docker Compose</strong>: Practice using Docker Compose for multi-container applications. Understanding how to define and run multi-container Docker applications can significantly streamline your workflow.</p>\n",
       "</li>\n",
       "<li><p><strong>Familiarize with Terraform</strong>: Learn the basics of Infrastructure as Code (IaC) with Terraform. Know how to define your infrastructure as code, including managing resources like servers and databases.</p>\n",
       "</li>\n",
       "<li><p><strong>Common Errors</strong>: Pay attention to common troubleshooting techniques, such as resolving <code>ModuleNotFoundError</code> for dependencies like <code>psycopg2</code>. Make sure to install necessary packages using <code>pip install</code> and keep them updated.</p>\n",
       "</li>\n",
       "<li><p><strong>Experiment with Examples</strong>: Try working through provided examples or creating your own small projects to solidify your understanding.</p>\n",
       "</li>\n",
       "<li><p><strong>Set Up Your Environment Properly</strong>: Ensure your development environment is set up correctly, including any paths or environment variables that may need configurations (e.g., setting up Python paths for packages).</p>\n",
       "</li>\n",
       "<li><p><strong>Ask Questions</strong>: If you encounter issues, don't hesitate to seek help from forums or classmates, and refer back to the course materials.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>By focusing on these areas, you can enhance your understanding and performance in Module 1.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to succeed in module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to succeed in module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_KhRLKv62AnuDHJeKztZKJgsU', 'output': '[\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Error raised during the jupyter notebook\\\\u2019s cell execution:\\\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\').\\\\nSolution: Need to install Python module \\\\u201cpsycopg2\\\\u201d. Can be installed by Conda or pip.\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named \\'psycopg2\\'.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"\\\\u2705SOLUTION: pip install confluent-kafka[avro].\\\\nFor some reason, Conda also doesn\\'t include this when installing confluent-kafka via pip.\\\\nMore sources on Anaconda and confluent-kafka issues:\\\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"ModuleNotFoundError: No module named \\'avro\\'\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\\\nTo create a virtual env and install packages (run only once)\\\\npython -m venv env\\\\nsource env/bin/activate\\\\npip install -r ../requirements.txt\\\\nTo activate it (you\\'ll need to run it every time you need the virtual env):\\\\nsource env/bin/activate\\\\nTo deactivate it:\\\\ndeactivate\\\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it\\'s env/Scripts/activate)\\\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Module \\\\u201ckafka\\\\u201d not found when trying to run producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"AttributeError: \\'DataFrame\\' object has no attribute \\'iteritems\\'\\\\nthis is because the method inside the pyspark refers to a package that has been already deprecated\\\\n(https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)\\\\nYou can do this code below, which is mentioned in the stackoverflow link above:\\\\nQ: DE Zoomcamp 5.6.3 - Setting up a Dataproc Cluster I cannot create a cluster and get this message. I tried many times as the FAQ said, but it didn\\'t work. What can I do?\\\\nError\\\\nInsufficient \\'SSD_TOTAL_GB\\' quota. Requested 500.0, available 250.0.\\\\nRequest ID: 17942272465025572271\\\\nA: The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\\\nMaster Node:\\\\nMachine type: n2-standard-2\\\\nPrimary disk size: 85 GB\\\\nWorker Node:\\\\nNumber of worker nodes: 2\\\\nMachine type: n2-standard-2\\\\nPrimary disk size: 80 GB\\\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Prefect: https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing\\\\nAirflow: https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing\",\\n    \"section\": \"Module 2: Workflow Orchestration\",\\n    \"question\": \"Where are the FAQ questions from the previous cohorts for the orchestration module?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"If below does not work, then download the latest available py4j version with\\\\nconda install -c conda-forge py4j\\\\nTake care of the latest version number in the website to replace appropriately.\\\\nNow add\\\\nexport PYTHONPATH=\\\\\"${SPARK_HOME}/python/:$PYTHONPATH\\\\\"\\\\nexport PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\\\\\"\\\\nin your  .bashrc file.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4J Error - ModuleNotFoundError: No module named \\'py4j\\' (Solve with latest version)\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"You can get the version of py4j from inside docker using this command\\\\ndocker exec -it --user airflow airflow-airflow-scheduler-1 bash -c \\\\\"ls /opt/spark/python/lib\\\\\"\",\\n    \"section\": \"Project\",\\n    \"question\": \"Spark docker - `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Many people aren\\\\u2019t getting an exact match, but are very close to one of the options. As per Alexey said to choose the closest option.\",\\n    \"section\": \"error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE.\",\\n    \"question\": \"Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Python 3.12.1, is not compatible with kafka-python-2.0.2. Therefore, instead of running \\\\\"pip install kafka-python\\\\\", you can resolve the issue by using \\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\". If you have already installed kafka-python, you need to run \\\\\"pip uninstall kafka-python\\\\\" before executing \\\\\"pip install git+https://github.com/dpkp/kafka-python.git\\\\\" to resolve the compatibility issue.\\\\nQ:In the Mage pipeline, individual blocks run successfully. However, when executing the pipeline as a whole, some blocks fail.\\\\nA: I have the following key-value pair in io_config.yaml file configured but still Mage blocks failed to generate OAuth and authenticate with GCP: GOOGLE_SERVICE_ACC_KEY_FILEPATH: \\\\\"{{ env_var(\\'GCP_CREDENTIALS\\') }}\\\\\". The GCP_CREDENTIALS variable holds the full path to the service account key\\'s JSON file. Adding the following line within the failed code block resolved the issue: os.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = os.environ.get(\\'GCP_CREDENTIALS\\').\\\\nThis occurs because the path to profiles.yml is not correctly specified. You can rectify this by:\\\\n\\\\u201cexport DBT_PROFILES_DBT=path/to/profiles.yml\\\\u201d\\\\nEg., /home/src/magic-zoomcamp/dbt/project_name/\\\\nDo the similar for DBT_PROJECT_DIR if getting similar issue with dbt_project.yml.\\\\nOnce DIRs are set,:\\\\n\\\\u201cdbt debug \\\\u2013config-dir\\\\u201d\\\\nThis would update your paths. To maintain same path across sessions, use the path variables in your .env file.\\\\nTo add triggers in mage pipelines via CLI, you can create a trigger of type API, and copy the API links.\\\\nEg. link: http://localhost:6789/api/pipeline_schedules/10/pipeline_runs/f3a1a4228fc64cfd85295b668c93f3b2\\\\nThen create a trigger.py as such:\\\\nimport os\\\\nimport requests\\\\nclass MageTrigger:\\\\nOPTIONS = {\\\\n\\\\\"<pipeline_name>\\\\\": {\\\\n\\\\\"trigger_id\\\\\": 10,\\\\n\\\\\"key\\\\\": \\\\\"f3a1a4228fc64cfd85295b668c93f3b2\\\\\"\\\\n}\\\\n}\\\\n@staticmethod\\\\ndef trigger_pipeline(pipeline_name, variables=None):\\\\ntrigger_id = MageTrigger.OPTIONS[pipeline_name][\\\\\"trigger_id\\\\\"]\\\\nkey = MageTrigger.OPTIONS[pipeline_name][\\\\\"key\\\\\"]\\\\nendpoint = f\\\\\"http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/{key}\\\\\"\\\\nheaders = {\\'Content-Type\\': \\'application/json\\'}\\\\npayload = {}\\\\nif variables is not None:\\\\npayload[\\'pipeline_run\\'] = {\\'variables\\': variables}\\\\nresponse = requests.post(endpoint, headers=headers, json=payload)\\\\nreturn response\\\\nMageTrigger.trigger_pipeline(\\\\\"<pipeline_name>\\\\\")\\\\nFinally, after the mage server is up an running, simply this command:\\\\npython trigger.py from mage directory in terminal.\\\\nCan I do data partitioning & clustering run by dbt pipeline, or I would need to do this manually in BigQuery afterwards?\\\\nYou can use this configuration in your DBT model:\\\\n{\\\\n\\\\\"field\\\\\": \\\\\"<field name>\\\\\",\\\\n\\\\\"data_type\\\\\": \\\\\"<timestamp | date | datetime | int64>\\\\\",\\\\n\\\\\"granularity\\\\\": \\\\\"<hour | day | month | year>\\\\\"\\\\n# Only required if data_type is \\\\\"int64\\\\\"\\\\n\\\\\"range\\\\\": {\\\\n\\\\\"start\\\\\": <int>,\\\\n\\\\\"end\\\\\": <int>,\\\\n\\\\\"interval\\\\\": <int>\\\\n}\\\\n}\\\\nand for clustering\\\\n{{\\\\nconfig(\\\\nmaterialized = \\\\\"table\\\\\",\\\\ncluster_by = \\\\\"order_id\\\\\",\\\\n)\\\\n}}\\\\nmore details in: https://docs.getdbt.com/reference/resource-configs/bigquery-configs\",\\n    \"section\": \"Triggers in Mage via CLI\",\\n    \"question\": \"Encountering the error \\\\\"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\\\\" when running \\\\\"from kafka import KafkaProducer\\\\\" in Jupyter Notebook for Module 6 Homework?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"According to https://github.com/dpkp/kafka-python/\\\\n\\\\u201cDUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE https://github.com/wbarnha/kafka-python-ng FOR THE TIME BEING\\\\u201d\\\\nUse pip install kafka-python-ng instead\",\\n    \"section\": \"Project\",\\n    \"question\": \"How to fix the error \\\\\"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\\\\\"?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To succeed in Module 1 (Docker and Terraform), here are some actionable steps:</p>\n",
       "<ol>\n",
       "<li><p><strong>Master the Basics</strong>: Make sure you understand the core concepts of Docker, including images, containers, Docker Compose, and Dockerfiles. Hands-on practice is key.</p>\n",
       "</li>\n",
       "<li><p><strong>Install Required Packages</strong>:</p>\n",
       "<ul>\n",
       "<li>If you encounter the <code>ModuleNotFoundError</code> for <code>psycopg2</code>, ensure to install it using:<pre><code class=\"language-bash\">pip install psycopg2-binary\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>Update it if you've previously installed it:<pre><code class=\"language-bash\">pip install psycopg2-binary --upgrade\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Using SQLAlchemy</strong>: If you experience errors with SQLAlchemy, verify your connection string. It should look something like this:</p>\n",
       "<pre><code class=\"language-python\">conn_string = &quot;postgresql+psycopg://username:password@localhost:5432/database&quot;\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Resolve Environment Issues</strong>:</p>\n",
       "<ul>\n",
       "<li>If there are issues with specific libraries (like <code>pytz</code>), you might need to add certain lines to your Dockerfile or install them explicitly.</li>\n",
       "<li>For example, you can add:<pre><code class=\"language-dockerfile\">RUN python -m pip install --no-cache pytz\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Experiment with Docker Compose</strong>: This will help you understand how to manage multi-container applications. Practice the commands <code>docker-compose build</code> and <code>docker-compose up</code>.</p>\n",
       "</li>\n",
       "<li><p><strong>Ask for Help</strong>: If you are stuck, don‚Äôt hesitate to consult your peers or the community forums. Often, others face similar issues and can provide quick solutions.</p>\n",
       "</li>\n",
       "<li><p><strong>Practice &amp; Experiment</strong>: Create small projects or try out different configurations in Docker and Terraform to reinforce your learning and build confidence.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>By focusing on these aspects, you‚Äôll be better equipped to excel in Module 1!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[47]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#Run activates chat loop\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mrunner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Agents/ai-bootcamp-rag-to-agents/.venv/lib/python3.12/site-packages/toyaikit/chat/runners.py:205\u001B[39m, in \u001B[36mOpenAIResponsesRunner.run\u001B[39m\u001B[34m(self, previous_messages, stop_criteria)\u001B[39m\n\u001B[32m    202\u001B[39m last_message_text = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    204\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m205\u001B[39m     question = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mchat_interface\u001B[49m\u001B[43m.\u001B[49m\u001B[43minput\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    206\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m question.lower() == \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    207\u001B[39m         \u001B[38;5;28mself\u001B[39m.chat_interface.display(\u001B[33m\"\u001B[39m\u001B[33mChat ended.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Agents/ai-bootcamp-rag-to-agents/.venv/lib/python3.12/site-packages/toyaikit/chat/interface.py:60\u001B[39m, in \u001B[36mIPythonChatInterface.input\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minput\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     question = \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mYou:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     61\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m question.strip()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Agents/ai-bootcamp-rag-to-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1275\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1273\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1274\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1275\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1276\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1277\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1278\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1279\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1280\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Agents/ai-bootcamp-rag-to-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1320\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1317\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1318\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1319\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1321\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1322\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:05:46.946385Z",
     "start_time": "2025-10-26T14:53:08.645842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adding a new tool which adds a Answer back to the Appendable index\n",
    "\n",
    "#\n",
    "def add_entry(question, answer):\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)\n",
    "\n",
    "\n",
    "add_entry_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"add_entry\",\n",
    "    \"description\": \"Add an entry to the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question to be added to the FAQ database\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The answer to the question\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n"
   ],
   "id": "7083320bcbdeea3f",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:05:46.947698Z",
     "start_time": "2025-10-26T14:53:11.736533Z"
    }
   },
   "cell_type": "code",
   "source": "agent_tools.add_tool(add_entry, add_entry_tool)",
   "id": "82dd85a4db721889",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:05:46.949650Z",
     "start_time": "2025-10-26T14:53:40.034243Z"
    }
   },
   "cell_type": "code",
   "source": "runner.run()",
   "id": "48e9bb20619b43ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To provide specific advice on excelling in Module 1, it's best to look for any guidelines or tips that might already be in the course FAQ. This could include study strategies, key topics to focus on, or common pitfalls to avoid. I'll check the FAQ database for relevant information on how to do well in Module 1.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to do well in Module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to do well in Module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_VLpqHfiwC1o6W3RynJDvGTU3', 'output': '[\\n  {\\n    \"text\": \"Issue:\\\\ne\\\\u2026\\\\nSolution:\\\\npip install psycopg2-binary\\\\nIf you already have it, you might need to update it:\\\\npip install psycopg2-binary --upgrade\\\\nOther methods, if the above fails:\\\\nif you are getting the \\\\u201c ModuleNotFoundError: No module named \\'psycopg2\\' \\\\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\\\nFirst uninstall the psycopg package\\\\nThen update conda or pip\\\\nThen install psycopg again using pip.\\\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Postgres - ModuleNotFoundError: No module named \\'psycopg2\\'\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\\n    \"section\": \"Module 4: analytics engineering with dbt\",\\n    \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Error raised during the jupyter notebook\\\\u2019s cell execution:\\\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\').\\\\nSolution: Need to install Python module \\\\u201cpsycopg2\\\\u201d. Can be installed by Conda or pip.\",\\n    \"section\": \"Module 1: Docker and Terraform\",\\n    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named \\'psycopg2\\'.\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>It seems that the FAQ does not specifically address general strategies for excelling in Module 1, but I can offer some tips based on common practices for mastering a course module, especially one focused on technical content like Docker and Terraform:</p>\n",
       "<ol>\n",
       "<li><p><strong>Understand the Basics</strong>: Ensure you have a strong grasp of the foundational concepts of Docker and Terraform. Utilize online resources or documentation if needed.</p>\n",
       "</li>\n",
       "<li><p><strong>Hands-On Practice</strong>: Work on practical exercises and projects. Set up your own Docker containers and practice deploying infrastructure with Terraform.</p>\n",
       "</li>\n",
       "<li><p><strong>Participate in Discussions</strong>: Engage with fellow students in forums or study groups to clarify doubts and gain different perspectives.</p>\n",
       "</li>\n",
       "<li><p><strong>Utilize Resources</strong>: Make sure to read through all the supplementary materials provided in the module. This might include tutorials, videos, and documentation.</p>\n",
       "</li>\n",
       "<li><p><strong>Ask Questions</strong>: If you're stuck, don't hesitate to reach out to your instructor or ask in forums. There's no shame in seeking help when needed.</p>\n",
       "</li>\n",
       "<li><p><strong>Study Regularly</strong>: Instead of cramming, try to study a little each day to reinforce your understanding.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Using these strategies can help you perform well in Module 1. If you have a specific aspect of the module you're struggling with, feel free to ask!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The search results primarily focus on specific technical issues related to Module 1, such as module errors and installation problems, rather than general tips for excelling in the module.</p>\n",
       "<p>However, if you're dealing with issues like missing Python modules or installation errors mentioned in the results, here are some focused pointers:</p>\n",
       "<ol>\n",
       "<li><p><strong>Ensure Dependencies are Installed</strong>: If you're encountering errors like &quot;ModuleNotFoundError&quot; for libraries like <code>psycopg2</code> or <code>pytz</code>, make sure to install them using pip or conda as described. For example:</p>\n",
       "<ul>\n",
       "<li>Use <code>pip install psycopg2-binary</code> or <code>pip install pytz</code>.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Check Configuration</strong>: Follow suggested resolutions in the results for configuring your environment properly, like ensuring your connection strings and paths are correctly set up.</p>\n",
       "</li>\n",
       "<li><p><strong>Debugging Errors</strong>: If you encounter specific errors while executing code (e.g., issues with SQLAlchemy), refer to the provided solutions in the results.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you have any specific errors or issues from Module 1 you would like assistance with, let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here‚Äôs a summary of the search results related to Module 1:</p>\n",
       "<ol>\n",
       "<li><p><strong>ModuleNotFoundError for <code>psycopg2</code></strong>:</p>\n",
       "<ul>\n",
       "<li>If you encounter this error, you may need to install the <code>psycopg2-binary</code> package using:<pre><code class=\"language-bash\">pip install psycopg2-binary\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>If you already have it installed but face the error, try updating it:<pre><code class=\"language-bash\">pip install psycopg2-binary --upgrade\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>TypeError with SQLAlchemy</strong>:</p>\n",
       "<ul>\n",
       "<li>If you get an error stating that a module object is not callable, make sure your connection string is correctly formatted. Use this format:<pre><code class=\"language-python\">conn_string = &quot;postgresql+psycopg://root:root@localhost:5432/ny_taxi&quot;\n",
       "engine = create_engine(conn_string)\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>General Errors in Jupyter Notebooks</strong>:</p>\n",
       "<ul>\n",
       "<li>You may run into issues with executing cells. Installing the necessary Python modules using conda or pip can help resolve these.</li>\n",
       "</ul>\n",
       "</li>\n",
       "</ol>\n",
       "<p>These results primarily deal with installation and configuration issues, essential for troubleshooting in Module 1. If you need more detailed guidance on any of these topics, feel free to ask!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The search returned a total of <strong>five results</strong> related to Module 1.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mrunner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Agents/ai-bootcamp-rag-to-agents/.venv/lib/python3.12/site-packages/toyaikit/chat/runners.py:205\u001B[39m, in \u001B[36mOpenAIResponsesRunner.run\u001B[39m\u001B[34m(self, previous_messages, stop_criteria)\u001B[39m\n\u001B[32m    202\u001B[39m last_message_text = \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    204\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m205\u001B[39m     question = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mchat_interface\u001B[49m\u001B[43m.\u001B[49m\u001B[43minput\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    206\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m question.lower() == \u001B[33m\"\u001B[39m\u001B[33mstop\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    207\u001B[39m         \u001B[38;5;28mself\u001B[39m.chat_interface.display(\u001B[33m\"\u001B[39m\u001B[33mChat ended.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Agents/ai-bootcamp-rag-to-agents/.venv/lib/python3.12/site-packages/toyaikit/chat/interface.py:60\u001B[39m, in \u001B[36mIPythonChatInterface.input\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     59\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minput\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m60\u001B[39m     question = \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mYou:\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     61\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m question.strip()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Agents/ai-bootcamp-rag-to-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1275\u001B[39m, in \u001B[36mKernel.raw_input\u001B[39m\u001B[34m(self, prompt)\u001B[39m\n\u001B[32m   1273\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1274\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[32m-> \u001B[39m\u001B[32m1275\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1276\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1277\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1278\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mshell\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1279\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1280\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Agents/ai-bootcamp-rag-to-agents/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1320\u001B[39m, in \u001B[36mKernel._input_request\u001B[39m\u001B[34m(self, prompt, ident, parent, password)\u001B[39m\n\u001B[32m   1317\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[32m   1318\u001B[39m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[32m   1319\u001B[39m     msg = \u001B[33m\"\u001B[39m\u001B[33mInterrupted by user\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1321\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1322\u001B[39m     \u001B[38;5;28mself\u001B[39m.log.warning(\u001B[33m\"\u001B[39m\u001B[33mInvalid Message:\u001B[39m\u001B[33m\"\u001B[39m, exc_info=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:16:28.297825Z",
     "start_time": "2025-10-26T15:16:28.291718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class SearchTools:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search the FAQ database for entries matching the given query.\n",
    "\n",
    "        Args:\n",
    "            query (str): Search query text to look up in the course FAQ.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "        \"\"\"\n",
    "        boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "        results = self.index.search(\n",
    "            query=query,\n",
    "            filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "            boost_dict=boost,\n",
    "            num_results=5,\n",
    "            output_ids=True\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def add_entry(self, question: str, answer: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a new entry to the FAQ database.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to be added to the FAQ database.\n",
    "            answer (str): The corresponding answer to the question.\n",
    "        \"\"\"\n",
    "        doc = {\n",
    "            'question': question,\n",
    "            'text': answer,\n",
    "            'section': 'user added',\n",
    "            'course': 'data-engineering-zoomcamp'\n",
    "        }\n",
    "        self.index.append(doc)\n"
   ],
   "id": "4ecf08fb2199aea1",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:20:58.823158Z",
     "start_time": "2025-10-26T15:19:37.249283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_tools = SearchTools(index)\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tools(search_tools)\n",
    "\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=instructions,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")\n",
    "\n",
    "runner.run();"
   ],
   "id": "415c9c5b30902a4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To provide the best answer, I need to clarify what &quot;it&quot; refers to. This could relate to running a specific library, a script, or a function in Python.</p>\n",
       "<p>Since there‚Äôs no specific context provided, I‚Äôll search the FAQ for general guidance on running Python code. This should give us a helpful overview or relevant information. Let me do that now.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"run Python\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"run Python\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_m4Lrcw76Wcq7x2c3DWoV4y4N', 'output': '[\\n  {\\n    \"text\": \"I found this error while executing the user defined function in Spark (crazy_stuff_udf). I am working on Windows and using conda. After following the setup instructions, I found that the PYSPARK_PYTHON environment variable was not set correctly, given that conda has different python paths for each environment.\\\\nSolution:\\\\npip install findspark on the command line inside proper environment\\\\nAdd to the top of the script\\\\nimport findspark\\\\nfindspark.init()\",\\n    \"section\": \"Module 5: pyspark\",\\n    \"question\": \"PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 318\\n  },\\n  {\\n    \"text\": \"You need to redefine the python environment variable to that of your user account\",\\n    \"section\": \"Project\",\\n    \"question\": \"How to run python as start up script?\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 398\\n  },\\n  {\\n    \"text\": \"If you get an error while running the command python3 stream.py worker\\\\nRun pip uninstall kafka-python\\\\nThen run pip install kafka-python==1.4.6\\\\nWhat is the use of  Redpanda ?\\\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka\\\\u00ae APIs while eliminating Kafka complexity.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Error while running python3 stream.py worker\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 375\\n  },\\n  {\\n    \"text\": \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\\\nTo create a virtual env and install packages (run only once)\\\\npython -m venv env\\\\nsource env/bin/activate\\\\npip install -r ../requirements.txt\\\\nTo activate it (you\\'ll need to run it every time you need the virtual env):\\\\nsource env/bin/activate\\\\nTo deactivate it:\\\\ndeactivate\\\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it\\'s env/Scripts/activate)\\\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\\n    \"section\": \"Module 6: streaming with kafka\",\\n    \"question\": \"Module \\\\u201ckafka\\\\u201d not found when trying to run producer.py\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 372\\n  },\\n  {\\n    \"text\": \"Below I have listed some steps I took to rectify this and potentially other minor errors, in Windows:\\\\nUse the git bash terminal in windows.\\\\nActivate python venv from git bash: source .venv/Scripts/activate\\\\nModify the seed_kafka.py file: in the first line, replace python3 with python.\\\\nNow from git bash, run the seed-kafka cmd. It should work now.\\\\nAdditional Notes:\\\\nYou can connect to the RisingWave cluster from Powershell with the command psql -h localhost -p 4566 -d dev -U root , otherwise it asks for a password.\\\\nThe equivalent of source commands.sh  in Powershell is . .\\\\\\\\commands.sh from the workshop directory.\\\\nHope this can save you from some trouble in case you\\'re doing this workshop on Windows like I am.\\\\n\\\\u2014--------------------------------------------------------------------------------------\",\\n    \"section\": \"Workshop 2 - RisingWave\",\\n    \"question\": \"Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.\",\\n    \"course\": \"data-engineering-zoomcamp\",\\n    \"_id\": 425\\n  }\\n]'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here are some insights from the FAQ that could help you understand how to run Python code:</p>\n",
       "<ol>\n",
       "<li><p><strong>Creating a Virtual Environment</strong>: It's recommended to create a virtual environment to manage dependencies effectively. Here‚Äôs how:</p>\n",
       "<ul>\n",
       "<li>To create a virtual environment (run only once):<pre><code class=\"language-bash\">python -m venv env\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>To activate it:<ul>\n",
       "<li>On MacOS/Linux:<pre><code class=\"language-bash\">source env/bin/activate\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>On Windows:<pre><code class=\"language-bash\">env\\Scripts\\activate\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li>Once activated, install the necessary packages using:<pre><code class=\"language-bash\">pip install -r requirements.txt\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Running Python Scripts</strong>: You can execute a Python script from the terminal/command line by using:</p>\n",
       "<pre><code class=\"language-bash\">python script_name.py\n",
       "</code></pre>\n",
       "<p>Replace <code>script_name.py</code> with the name of your script.</p>\n",
       "</li>\n",
       "<li><p><strong>Common Errors and Solutions</strong>: If you encounter errors, such as &quot;Python not found,&quot; it may be necessary to ensure your environment variables are set correctly or that you‚Äôre using the appropriate Python executable (e.g., <code>python3</code> instead of <code>python</code>).</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you have a specific task or code snippet you're trying to run, please share more details, and I can provide a more tailored response.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>add_entry({\"question\":\"How to run Python code in a virtua...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"question\":\"How to run Python code in a virtual environment?\",\"answer\":\"To run Python code effectively in a virtual environment, follow these steps:\\n1. Create a virtual environment (run only once):\\n   ```bash\\n   python -m venv env\\n   ```\\n2. Activate it:\\n   - On MacOS/Linux:\\n     ```bash\\n     source env/bin/activate\\n     ```\\n   - On Windows:\\n     ```bash\\n     env\\\\Scripts\\\\activate\\n     ```\\n3. Install necessary packages:\\n   ```bash\\n   pip install -r requirements.txt\\n   ```\\n4. Run your Python script from the terminal:\\n   ```bash\\n   python script_name.py\\n   ```\\nReplace `script_name.py` with your actual script name.\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>{'type': 'function_call_output', 'call_id': 'call_cnM4P68Kjp3UvTb0HbyTm1vs', 'output': 'null'}</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The information has been successfully added to the FAQ. Here's a summary of what was included:</p>\n",
       "<h3>How to run Python code in a virtual environment?</h3>\n",
       "<p>To run Python code effectively in a virtual environment, follow these steps:</p>\n",
       "<ol>\n",
       "<li><p><strong>Create a virtual environment (run only once)</strong>:</p>\n",
       "<pre><code class=\"language-bash\">python -m venv env\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Activate it</strong>:</p>\n",
       "<ul>\n",
       "<li>On MacOS/Linux:<pre><code class=\"language-bash\">source env/bin/activate\n",
       "</code></pre>\n",
       "</li>\n",
       "<li>On Windows:<pre><code class=\"language-bash\">env\\Scripts\\activate\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Install necessary packages</strong>:</p>\n",
       "<pre><code class=\"language-bash\">pip install -r requirements.txt\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Run your Python script from the terminal</strong>:</p>\n",
       "<pre><code class=\"language-bash\">python script_name.py\n",
       "</code></pre>\n",
       "<p>Replace <code>script_name.py</code> with your actual script name.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>If you have any other topics or questions you'd like to add, feel free to let me know!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "execution_count": 58
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
